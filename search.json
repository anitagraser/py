[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geocomputation with Python",
    "section": "",
    "text": "This site contains ideas, code and an outline of a yet-to-be written book on Geocomputation with Python.\n\n\nGeocomputation with Python, is motivated by the need for an introductory yet rigorous and maintained resource on working with geographic data in Python. A unique feature of the book is that it that demonstrates code for working with both vector and raster geographic data types.  There are many resources on Python packages for geographic research and various applications but, to the best of our knowledge, no other resource brings together the following features into a single home:\n\nSmall introductory textbook focuses on doing basic operations well\nIntegration of vector and raster datasets in the same book, and within each section\nClear explanation of the code and exercises to maximize learning for newcomers\nProvision of lucid example datasets and meaningful operations to illustrate the applied nature of geographic research\n\nThe book aims to supplement other resources in the ecosystem, as highlighted by comparison with the book’s scope with existing and in-progress works:\n\nLearning Geospatial Analysis with Python and Geoprocessing with Python focuses on processing spatial data using low-level Python interfaces for GDAL, such as the gdal, gdalnumeric, and ogr packages from osgeo. This approach is more complex, less “Pythonic”, and perhaps outdated in light of development of packages such as geopandas and rasterio covered here\npythongis.org (at an early stage of development) seeks to provide a general introduction to ‘GIS in Python’, with parts focusing on Python essentials, using Python with GIS, and case studies. Compared with pythongis.org, geocompy has a relatively narrow scope (1) and a greater focus on raster-vector interoperability\ngeographicdata.science is an ambitious project with chapters dedicated to advanced topics, with Chapter 4 on Spatial Weights getting into complex topics relatively early, for example. Geocompy would be shorter, simpler and more introductory, and cover raster and vector data with equal importance (1 to 4)\n\nGeocompy is a sister project of Geocomputation with R – a book on geographic data analysis, visualization, and modeling using the R programming language.\n\n\n\nAn important aspect of scientific research and ‘citizen science’ that is participatory is reproducibility of results.\n\n\nTo reproduce this book you can simply click on the link below to see the code running in your web browser (see details of how this works at mybinder.org):\n\n\n\nBinder\n\n\n\n\n\nTo run the code locally, recommended for using the material on real data, you need to have a reasonable computer, e.g. with 8 GB RAM. You’ll need administrative rights to install the requirements, which include:\n\nA suitable integrated development environment (IDE) such as VS Code, RStudio or Jupyter Notebook\nQuarto, if you want to reproduce the book’s open access website\nEither an Anaconda-like environment (we recommend miniconda3) or Docker to get systems dependencies\n\nSee the project’s README for details on getting set-up. After you have installed the necessary dependencies and cloned or unzipped the book’s source code, you should be able to reproduce the code in its entirety with the following command:\nquarto preview\nIf you see output like that below (with the IDE and browser arranged to see live updates after editing the source code), congratulations, it has worked!\n\n\n\n\nAlternatively, you can download and unzip the book’s source code. The unzipped directory py-main/code/chapters/ contains:\n\nThe source ipynb files, one for each Chapter\nThe data sub-directory with the sample data used in the code sections\n\nAssuming that all required packages are installed (see beginning of each chapter), you can execute the ipynb files through your chosen working environment (VScode, Jupyter Notebook, etc.)."
  },
  {
    "objectID": "02-spatial-data.html",
    "href": "02-spatial-data.html",
    "title": "2  Geographic data in Python",
    "section": "",
    "text": "This chapter introduces key Python packages and data structures for working with the two major types of spatial data, namely:\n\nshapely and geopandas — for working with vector layers\nrasterio and xarray — for working with rasters\n\nAs we will see in the example code presented later in this chapter, shapely and geopandas are related:\n\nshapely is a “low-level” package for working with individual vector geometry objects\ngeopandas is a “high-level” package for working with geometry columns (GeoSeries objects), which internally contain shapely geometries, and vector layers (GeoDataFrame objects)\n\nThe geopandas ecosystem provides a comprehensive approach for working with vector layers in Python, with many packages building on it. This is not the case for raster data, however: there are several partially overlapping packages for working with raster data, each with its own advantages and disadvantages. In this book we focus on the most prominent one:\n\nrasterio — a spatial-oriented package, focused on “simple” raster formats (such as GeoTIFF), representing a raster using a combination of a numpy array, and a metadata object (dict) specifying the spatial referencing of the array\n\nAnother raster-related package worth mentioning is xarray. It is a general-purpose package for working with labeled arrays, thus advantageous for processing “complex” raster format (such as NetCDF), representing a raster using its own native classes, namely xarray.Dataset and xarray.DataArray\nThis chapter will briefly explain the fundamental geographic data models: vector and raster. Before demonstrating their implementation in Python, we will introduce the theory behind each data model and the disciplines in which they predominate.\nThe vector data model represents the world using points, lines, and polygons. These have discrete, well-defined borders, meaning that vector datasets usually have a high level of precision (but not necessarily accuracy).  The raster data model divides the surface up into cells of constant size. Raster datasets are the basis of background images used in web-mapping and have been a vital source of geographic data since the origins of aerial photography and satellite-based remote sensing devices. Rasters aggregate spatially specific features to a given resolution, meaning that they are consistent over space and scalable (many worldwide raster datasets are available).\nWhich to use? The answer likely depends on your domain of application, and the datasets you have access to:\n\nVector datasets and methods dominate the social sciences because human settlements and and processes (e.g. transport infrastructure) tend to have discrete borders\nRaster datasets and methods dominate many environmental sciences because of the reliance on remote sensing data\n\nThere is much overlap in some fields and raster and vector datasets can be used together: ecologists and demographers, for example, commonly use both vector and raster data. Furthermore, it is possible to convert between the two forms  Whether your work involves more use of vector or raster datasets, it is worth understanding the underlying data models before using them, as discussed in subsequent chapters. This book focusses on approaches that build on geopandas and rasterio packages to work with vector data and raster datasets, respectively."
  },
  {
    "objectID": "02-spatial-data.html#vector-data",
    "href": "02-spatial-data.html#vector-data",
    "title": "2  Geographic data in Python",
    "section": "2.2 Vector data",
    "text": "2.2 Vector data\nThe geographic vector data model is based on points located within a coordinate reference system (CRS). Points can represent self-standing features (e.g., the location of a bus stop), or they can be linked together to form more complex geometries such as lines and polygons. Most point geometries contain only two dimensions (3-dimensional CRSs contain an additional \\(z\\) value, typically representing height above sea level).\nIn this system, London, for example, can be represented by the coordinates (-0.1, 51.5). This means that its location is -0.1 degrees east and 51.5 degrees north of the origin. The origin, in this case, is at 0 degrees longitude (the Prime Meridian) and 0 degree latitude (the Equator) in a geographic (‘lon/lat’) CRS.  The same point could also be approximated in a projected CRS with ‘Easting/Northing’ values of (530000, 180000) in the British National Grid, meaning that London is located 530 \\(km\\) East and 180 \\(km\\) North of the origin of the CRS.   The location of National Grid’s origin, in the sea beyond South West Peninsular, ensures that most locations in the UK have positive Easting and Northing values.\n\ngeopandas provides classes for geographic vector data and a consistent command-line interface for reproducible geographic data analysis in Python. geopandas provides an interface to three mature libraries for geocomputation which, in combination, represent a strong foundation on which many geographic applications (including QGIS and R’s spatial ecosystem) builds:\n\nGDAL, for reading, writing, and manipulating a wide range of geographic data formats, covered in Chapter 8\nPROJ, a powerful library for coordinate system transformations, which underlies the content covered in Chapter 7\nGEOS, a planar geometry engine for operations such as calculating buffers and centroids on data with a projected CRS, covered in Chapter 5\n\nTight integration with these geographic libraries makes reproducible geocomputation possible: an advantage of using a higher level language such as Python to access these libraries is that you do not need to know the intricacies of the low level components, enabling focus on the methods rather than the implementation. This section introduces geopandas classes in preparation for subsequent chapters (Chapters 5 and 8 cover the GEOS and GDAL interface, respectively).\n\n2.2.1 Vector data classes\nThe main classes for working with geographic vector data in Python are hierarchical, meaning the highest level ‘vector layer’ class is composed of simpler ‘geometry column’ and individual ‘geometry’ components. This section introduces them in order, starting with the highest level class. For many applications, the high level vector layer class, which are essentially a data frame with geometry columns, are all that’s needed. However, it’s important to understand the structure of vector geographic objects and their component pieces for more advanced applications. The three main vector geographic data classes in Python are:\n\nGeoDataFrame, a class representing vector layers, with a geometry column (class GeoSeries) as one of the columns\nGeoSeries, a class that is used to represent the geometry column in GeoDataFrame objects\nshapely geometry objects which represent individual geometries, such as a point or a polygon\n\nThe first two classes (GeoDataFrame and GeoSeries) are defined in geopandas. The third class is defined in the shapely package, which deals with individual geometries, and is a main dependency of the geopandas package.\n\n\n2.2.2 Vector layers\nThe most commonly used geographic vector data structure is the vector layer. There are several approaches for working with vector layers in Python, ranging from low-level packages (e.g., osgeo, fiona) to the relatively high-level geopandas package that is the focus of this section. Before writing and running code for creating and working with geographic vector objects, we therefore import geopandas (by convention as gpd for more concise code) and shapely:\nimport geopandas as gpd\nimport shapely.geometry\nimport shapely.wkt\nWe also limit the maximum number of printed rows to four, to save space, using the 'display.max_rows' option of pandas:\nimport pandas as pd\npd.set_option('display.max_rows', 4)\nProjects often start by importing an existing vector layer saved as an ESRI Shapefile (.shp), a GeoPackage (.gpkg) file, or other geographic file format. The function read_file() in the following line of code imports a GeoPackage file named world.gpkg located in the data directory of Python’s working directory into a GeoDataFrame named gdf:\ngdf = gpd.read_file('data/world.gpkg')\nAs result is an object of type (class) GeoDataFrame with 177 rows (features) and 11 columns, as shown in the output of the following code:\n\ntype(gdf)\ngdf.shape\n\n(177, 11)\n\n\nThe GeoDataFrame class is an extension of the DataFrame class from the popular pandas package. This means we can treat a vector layer as a table, and process it using the ordinary, i.e., non-spatial, established function methods. For example, standard data frame subsetting methods can be used. The code below creates a subset of the gdf dataset containing only the country name and the geometry:\n\ngdf = gdf[['name_long', 'geometry']]\ngdf\n\n\n\n\n\n  \n    \n      \n      name_long\n      geometry\n    \n  \n  \n    \n      0\n      Fiji\n      MULTIPOLYGON (((-180.00000 -16.55522, -179.917...\n    \n    \n      1\n      Tanzania\n      MULTIPOLYGON (((33.90371 -0.95000, 31.86617 -1...\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      175\n      Trinidad and Tobago\n      MULTIPOLYGON (((-61.68000 10.76000, -61.66000 ...\n    \n    \n      176\n      South Sudan\n      MULTIPOLYGON (((30.83385 3.50917, 31.24556 3.7...\n    \n  \n\n177 rows × 2 columns\n\n\n\nThe following expression creates a subset based on a condition, such as equality of the value in the 'name_long' column to the string 'Egypt':\n\ngdf[gdf['name_long'] == 'Egypt']\n\n\n\n\n\n  \n    \n      \n      name_long\n      geometry\n    \n  \n  \n    \n      163\n      Egypt\n      MULTIPOLYGON (((36.86623 22.00000, 36.69069 22...\n    \n  \n\n\n\n\nFinally, to get a sense of the spatial component of the vector layer, it can be plotted using the .plot method, as follows:\n\ngdf.plot();\n\n\n\n\nor using .explore to get an interactive plot:\n\ngdf.explore()\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\nAnd consequently, a subset can be plotted using:\n\ngdf[gdf['name_long'] == 'Egypt'].explore()\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n2.2.3 Geometry columns\nA vital column in a GeoDataFrame is the geometry column, of class GeoSeries. The geometry column contains the geometric part of the vector layer. In the case of the gdf object, the geometry column contains 'MultiPolygon's associated with each country:\n\ngdf['geometry']\n\n0      MULTIPOLYGON (((-180.00000 -16.55522, -179.917...\n1      MULTIPOLYGON (((33.90371 -0.95000, 31.86617 -1...\n                             ...                        \n175    MULTIPOLYGON (((-61.68000 10.76000, -61.66000 ...\n176    MULTIPOLYGON (((30.83385 3.50917, 31.24556 3.7...\nName: geometry, Length: 177, dtype: geometry\n\n\nThe geometry column also contains the spatial reference information, if any:\n\ngdf['geometry'].crs\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nMany geometry operations, such as calculating the centroid, buffer, or bounding box of each feature involve just the geometry. Applying this type of operation on a GeoDataFrame is therefore basically a shortcut to applying it on the GeoSeries object in the geometry column. The two following commands therefore return exactly the same result, a GeoSeries with country centroids:\n\ngdf.centroid\n\n/tmp/ipykernel_602/2017122361.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf.centroid\n\n\n0      POINT (163.85312 -17.31631)\n1        POINT (34.75299 -6.25773)\n                  ...             \n175     POINT (-61.33037 10.42824)\n176       POINT (30.19862 7.29289)\nLength: 177, dtype: geometry\n\n\n\ngdf['geometry'].centroid\n\n/tmp/ipykernel_602/3996546279.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf['geometry'].centroid\n\n\n0      POINT (163.85312 -17.31631)\n1        POINT (34.75299 -6.25773)\n                  ...             \n175     POINT (-61.33037 10.42824)\n176       POINT (30.19862 7.29289)\nLength: 177, dtype: geometry\n\n\n\nAnother useful property of the geometry column is the geometry type, as shown in the following code. Note that the types of geometries contained in a geometry column (and, thus, a vector layer) are not necessarily the same for every row. Accordingly, the .type property returns a Series (of type string), rather than a single value:\n\ngdf['geometry'].type\n\n0      MultiPolygon\n1      MultiPolygon\n           ...     \n175    MultiPolygon\n176    MultiPolygon\nLength: 177, dtype: object\n\n\nTo summarize the occurrence of different geometry types in a geometry column, we can use the pandas method called value_counts:\n\ngdf['geometry'].type.value_counts()\n\nMultiPolygon    177\ndtype: int64\n\n\nIn this case, we see that the gdf layer contains only 'MultiPolygon' geometries. It is possible to have multiple geometry types in a single GeoSeries and a GeoDataFrame can have multiple GeoSeries:\n\ngdf['centroids'] = gdf.centroid\ngdf['polygons'] = gdf.geometry\ngdf\n\n/tmp/ipykernel_602/104973583.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf['centroids'] = gdf.centroid\n\n\n\n\n\n\n  \n    \n      \n      name_long\n      geometry\n      centroids\n      polygons\n    \n  \n  \n    \n      0\n      Fiji\n      MULTIPOLYGON (((-180.00000 -16.55522, -179.917...\n      POINT (163.85312 -17.31631)\n      MULTIPOLYGON (((-180.00000 -16.55522, -179.917...\n    \n    \n      1\n      Tanzania\n      MULTIPOLYGON (((33.90371 -0.95000, 31.86617 -1...\n      POINT (34.75299 -6.25773)\n      MULTIPOLYGON (((33.90371 -0.95000, 31.86617 -1...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      175\n      Trinidad and Tobago\n      MULTIPOLYGON (((-61.68000 10.76000, -61.66000 ...\n      POINT (-61.33037 10.42824)\n      MULTIPOLYGON (((-61.68000 10.76000, -61.66000 ...\n    \n    \n      176\n      South Sudan\n      MULTIPOLYGON (((30.83385 3.50917, 31.24556 3.7...\n      POINT (30.19862 7.29289)\n      MULTIPOLYGON (((30.83385 3.50917, 31.24556 3.7...\n    \n  \n\n177 rows × 4 columns\n\n\n\nTo switch the geometry column from one `GeoSeries column to another, we use set_geometry:\n\ngdf.set_geometry('centroids', inplace=True)\ngdf.explore()\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\ngdf.set_geometry('polygons', inplace=True)\ngdf.explore()\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n2.2.4 The Simple Features standard\nGeometries are the basic building blocks of vector layers. Although the Simple Features standard defines about 20 types of geometries, we will focus on the seven most commonly used types: POINT, LINESTRING, POLYGON, MULTIPOINT, MULTILINESTRING, MULTIPOLYGON and GEOMETRYCOLLECTION. Find the whole list of possible feature types in the PostGIS manual. \nWell-known binary (WKB) and well-known text (WKT) are the standard encodings for simple feature geometries.  WKB representations are usually hexadecimal strings easily readable for computers. This is why GIS and spatial databases use WKB to transfer and store geometry objects. WKT, on the other hand, is a human-readable text markup description of simple features. Both formats are exchangeable, and if we present one, we will naturally choose the WKT representation.\nThe foundation of each geometry type is the point. A point is simply a coordinate in 2D, 3D, or 4D space such as: \nPOINT (5 2)\nA linestring is a sequence of points with a straight line connecting the points, for example: \nLINESTRING (1 5, 4 4, 4 1, 2 2, 3 2)\nA polygon is a sequence of points that form a closed, non-intersecting ring. Closed means that the first and the last point of a polygon have the same coordinates (see right panel in Figure …).\nPOLYGON ((1 5, 2 2, 4 1, 4 4, 1 5))\n\nSo far we have created geometries with only one geometric entity per feature. However, the Simple Features standard allows multiple geometries to exist within a single feature, using “multi” versions of each geometry type, as illustrated below:\nMULTIPOINT (5 2, 1 3, 3 4, 3 2)\nMULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4))\nMULTIPOLYGON (((1 5, 2 2, 4 1, 4 4, 1 5), (0 2, 1 2, 1 3, 0 3, 0 2)))\n\nFinally, a geometry collection can contain any combination of geometries including (multi)points and linestrings: \nGEOMETRYCOLLECTION (MULTIPOINT (5 2, 1 3, 3 4, 3 2), LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2))\n\n\n2.2.5 Geometries\nEach element in the geometry column is a geometry object, of class shapely. For example, here is one specific geometry selected by implicit index (that of Canada):\n\ngdf['geometry'].iloc[3]\n\n\n\n\nWe can also select a specific geometry based on the 'name_long' attribute:\n\ngdf[gdf['name_long'] == 'Egypt']['geometry'].iloc[0]\n\n\n\n\nThe shapely package is compatible with the Simple Features standard. Accordingly, seven types of geometries are supported. The following section demonstrates creating a shapely geometry of each type from scratch. In the first example (a 'Point') we demonstrate two types of inputs for geometry creation:\n\na list of coordinates\na string in the WKT format and\n\nIn the examples for the remaining geometries we use the former approach.\nCreating a 'Point' geometry from a list of coordinates uses the shapely.geometry.Point function:\n\npoint = shapely.geometry.Point([5, 2])\npoint\n\n\n\n\nAlternatively, we can use the shapely.wkt.loads (stands for “load a WKT string”) to transform a WKT string to a shapely geometry object. Here is an example of creating the same 'Point' geometry from WKT:\n\npoint = shapely.wkt.loads('POINT (5 2)')\npoint\n\n\n\n\nHere is an example of a 'MultiPoint' geometry from a list of coordinate tuples:\n\nmultipoint = shapely.geometry.MultiPoint([(5,2), (1,3), (3,4), (3,2)])\nmultipoint\n\n\n\n\nHere is an example of a 'LineString' geometry from a list of coordinate tuples:\n\nlinestring = shapely.geometry.LineString([(1,5), (4,4), (4,1), (2,2), (3,2)])\nlinestring\n\n\n\n\nHere is an example of a 'MultiLineString' geometry. Note that there is one list of coordinates for each line in the MultiLineString:\n\nlinestring = shapely.geometry.MultiLineString([[(1,5), (4,4), (4,1), (2,2), (3,2)], [(1,2), (2,4)]])\nlinestring\n\n\n\n\nHere is an example of a 'Polygon' geometry. Not that there is one list of coordinates that defines the exterior outer hull of the polygon, followed by a list of lists of coordinates that define the potential holes in the polygon:\n\npolygon = shapely.geometry.Polygon([(1,5), (2,2), (4,1), (4,4), (1,5)], [[(2,4), (3,4), (3,3), (2,3), (2,4)]])\npolygon\n\n\n\n\nHere is an example of a 'MultiPolygon' geometry:\n\nmultipolygon = shapely.geometry.MultiPolygon([\n    shapely.geometry.Polygon([(1,5), (2,2), (4,1), (4,4), (1,5)]), \n    shapely.geometry.Polygon([(0,2), (1,2), (1,3), (0,3), (0,2)])\n])\nmultipolygon\n\n\n\n\nAnd, finally, here is an example of a 'GeometryCollection' geometry:\n\nmultipoint = shapely.geometry.GeometryCollection([\n    shapely.geometry.MultiPoint([(5,2), (1,3), (3,4), (3,2)]),\n    shapely.geometry.MultiLineString([[(1,5), (4,4), (4,1), (2,2), (3,2)], [(1,2), (2,4)]])\n])\nmultipoint\n\n\n\n\nshapely geometries act as atomic units of vector data, as spatial operations on a geometry return a single new geometry. For example, the following expression calculates the difference between the buffered multipolygon (using distance of 0.2) and itself:\n\nmultipolygon.buffer(0.2).difference(multipolygon)\n\n\n\n\nAs demonstrated above, a shapely geometry object is automatically evaluated to a small image of the geometry (when using an interface capable of displaying it, such as a Jupyter Notebook). To print the WKT string instead, we can use the print function:\n\nprint(linestring)\n\nMULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4))\n\n\nWe can determine the geometry type using the .geom_type property, which returns a string:\n\nlinestring.geom_type\n\n'MultiLineString'\n\n\nFinally, it is important to note that raw coordinates of shapely geometries are accessible through a combination of the .coords, .geoms, .exterior, and .interiors, properties (depending on the geometry type). These access methods are helpful when we need to develop our own spatial operators for specific tasks. For example, the following expression returns the list of all coordinates of the polygon geometry exterior:\n\nlist(polygon.exterior.coords)\n\n[(1.0, 5.0), (2.0, 2.0), (4.0, 1.0), (4.0, 4.0), (1.0, 5.0)]\n\n\n\n\n2.2.6 Vector layer from scratch\nIn the previous sections we started with a vector layer (GeoDataFrame), from an existing Shapefile, and “decomposed” it to extract the geometry column (GeoSeries, Section 2.2.3) and separate geometries (shapely, see Section 2.2.5). In this section, we will demonstrate the opposite process, constructing a GeoDataFrame from shapely geometries, combined into a GeoSeries. This will:\n\nHelp you better understand the structure of a GeoDataFrame, and\nMay come in handy when you need to programmatically construct simple vector layers, such as a line between two given points, etc.\n\nVector layers consist of two main parts: geometries and non-geographic attributes. Figure … shows how a GeoDataFrame object is created – geometries come from a GeoSeries object, while attributes are taken from Series objects.\n(Figure such as https://geocompr.robinlovelace.net/spatial-class.html#fig:02-sfdiagram)\nNon-geographic attributes represent the name of the feature or other attributes such as measured values, groups, and other things. To illustrate attributes, we will represent a temperature of 25°C in London on June 21st, 2017. This example contains a geometry (the coordinates), and three attributes with three different classes (place name, temperature and date). Objects of class GeoDataFrame represent such data by combining the attributes (Series) with the simple feature geometry column (GeoSeries). First, we create a point geometry, which we know how to do from Section 2.2.5:\n\nlnd_point = shapely.geometry.Point(0.1, 51.5)\nlnd_point\n\n\n\n\nNext, we create a GeoSeries (of length 1), containing the point. Note that a GeoSeries stores a CRS definition, in this case WGS84 (defined using its EPSG code 4326). Also note that the shapely geometries go into a list, to illustrate that there can be more than one (unlike in this example):\n\nlnd_geom = gpd.GeoSeries([lnd_point], crs = 4326)\nlnd_geom\n\n0    POINT (0.10000 51.50000)\ndtype: geometry\n\n\nNext, we combine the GeoSeries with other attributes into a dict. The geometry column is a GeoSeries, named geometry. The other attributes (if any) may be defined using list or Series objects. Here, for simplicity, we use the list option for defining the three attributes name, temperature, and date. Again, note that the list can be of length >1, in case we are creating a layer with more than one feature:\nd = {\n  'name': ['London'],\n  'temperature': [25],\n  'date': ['2017-06-21'],\n  'geometry': lnd_geom\n}\nFinally, the dict can be coverted to a GeoDataFrame:\n\nlnd_layer = gpd.GeoDataFrame(d)\nlnd_layer\n\n\n\n\n\n  \n    \n      \n      name\n      temperature\n      date\n      geometry\n    \n  \n  \n    \n      0\n      London\n      25\n      2017-06-21\n      POINT (0.10000 51.50000)\n    \n  \n\n\n\n\nWhat just happened? First, the coordinates were used to create the simple feature geometry (shapely). Second, the geometry was converted into a simple feature geometry column (GeoSeries), with a CRS. Third, attributes were combined with GeoSeries. This results in an GeoDataFrame object, named lnd_layer.\nJust to illustrate how does creating a layer with more than one feature looks like, here is an example where we create a layer with two points, London and Paris:\n\nlnd_point = shapely.geometry.Point(0.1, 51.5)\nparis_point = shapely.geometry.Point(2.3, 48.9)\ntowns_geom = gpd.GeoSeries([lnd_point, paris_point], crs = 4326)\nd = {\n  'name': ['London', 'Paris'],\n  'temperature': [25, 27],\n  'date': ['2017-06-21', '2017-06-21'],\n  'geometry': towns_geom\n}\ntowns_layer = gpd.GeoDataFrame(d)\ntowns_layer\n\n\n\n\n\n  \n    \n      \n      name\n      temperature\n      date\n      geometry\n    \n  \n  \n    \n      0\n      London\n      25\n      2017-06-21\n      POINT (0.10000 51.50000)\n    \n    \n      1\n      Paris\n      27\n      2017-06-21\n      POINT (2.30000 48.90000)\n    \n  \n\n\n\n\nThe following expression creates an interactive map with the result:\n\ntowns_layer.explore()\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\nAlternatively, we can first create a pandas.DataFrame and turn it into a GeoDataFrame like this:\n\nimport pandas \ndf = pandas.DataFrame({\n  'name': ['London', 'Paris'],\n  'temperature': [25, 27],\n  'date': ['2017-06-21', '2017-06-21'],\n  'x': [0.1, 2.3],\n  'y': [51.5, 48.9]\n})\ngdf = gpd.GeoDataFrame(df, \n    geometry = gpd.points_from_xy(df['x'], df['y']), \n    crs = 'EPSG:4326')\ngdf.explore()\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\nThis approach is particularly useful when we need to read data from a CSV file, e.g., using pandas.read_csv, and want to turn the resulting DataFrame into a GeoDataFrame."
  },
  {
    "objectID": "02-spatial-data.html#raster-data",
    "href": "02-spatial-data.html#raster-data",
    "title": "2  Geographic data in Python",
    "section": "2.3 Raster data",
    "text": "2.3 Raster data\n\n2.3.1 Introduction\nAs mentioned above, working with rasters in Python is less organized around one comprehensive package (compared to the case for vector layers and geopandas). Instead, several packages provide alternative subsets of method for working with raster data.\nThe two most notable approaches for working with rasters in Python are provided by the rasterio and xarray packages. As we will see shortly, they differ in their scope and underlying data models. Specifically, rasterio represents rasters as numpy arrays associated with a separate object holding the spatial metadata. The xarray package, however, represents rasters with the native DataArray object, which is an extension of numpy array designed to hold axis labels and attributes, in the same object, together with the array of raster values.\nBoth packages are not exhaustive in the same way geopandas is. For example, when working with rasterio, on the one hand, more packages may be needed to accomplish common tasks such as zonal statistics (package rasterstats) or calculating topographic indices (package richdem). On the other hand, xarray was extended to accommodate spatial operators missing from the core package itself, with the rioxarray and xarray-spatial packages.\nIn the following two sections, we introduce rasterio, which is the raster-related package we are going to work with through the rest of the book.\n\n\n2.3.2 Using rasterio\nTo work with the rasterio package, we first need to import it. We also import numpy, since the underlying raster data are stored in numpy arrays. To effectively work with those, we expose all numpy functions. Finally, we import the show function from the rasterio.plot sub-module for quick visualization of rasters.\nimport numpy as np\nimport rasterio\nfrom rasterio.plot import show\nimport subprocess\nRasters are typically imported from existing files. When working with rasterio, importing a raster is actually a two-step process:\n\nFirst, we open a raster file “connection” using rasterio.open\nSecond, we read raster values from the connection using the .read method\n\nThis separation is analogous to basic Python functions for reading from files, such as open and .readline to read from a text file. The rationale is that we do not always want to read all information from the file into memory, which is particularly important as rasters size can be larger than RAM size. Accordingly, the second step (.read) is selective. For example, we may want to read just one raster band rather than reading all bands.\nIn the first step, we pass a file path to the rasterio.open function to create a DatasetReader file connection. For this example, we use a single-band raster representing elevation in Zion National Park:\n\nsrc = rasterio.open('data/srtm.tif')\nsrc\n\n<open DatasetReader name='data/srtm.tif' mode='r'>\n\n\nTo get a first impression of the raster values, we can plot the raster using the show function:\n\nshow(src);\n\n\n\n\nThe DatasetReader contains the raster metadata, that is, all of the information other than the raster values. Let us examine it:\n\nsrc.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint16',\n 'nodata': 65535.0,\n 'width': 465,\n 'height': 457,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.0008333333332777796, 0.0, -113.23958321278403,\n        0.0, -0.0008333333332777843, 37.512916763165805)}\n\n\nImportantly, we can see:\n\nThe raster data type (dtype)\nRaster dimensions (width, height, and count, i.e., number of layers)\nRaster Coordinate Reference System (crs)\nThe raster affine transformation matrix (transform)\n\nThe last item (i.e., transform) deserves more attention. To position a raster in geographical space, in addition to the CRS we must specify the raster origin (\\(x_{min}\\), \\(y_{max}\\)) and resolution (\\(delta_{x}\\), \\(delta_{y}\\)). In the transform matrix notation, these data items are stored as follows:\nAffine(delta_x, 0.0, x_min,\n       0.0, delta_y, y_max)\nNote that, by convention, raster y-axis origin is set to the maximum value (\\(y_{max}\\)) rather than the minimum, and, accordingly, the y-axis resolution (\\(delta_{y}\\)) is negative.\nFinally, the .read method of the DatasetReader is used to read the actual raster values. Importantly, we can read:\n\nA particular layer, passing a numeric index (as in .read(1))\nA subset of layers, passing a list of indices (as in .read([1,2]))\nAll layers (as in .read())\n\nNote that the layer indices start from 1, contrary to the Python convention of the first index being 0.\nThe resulting object is a numpy array, with either two or three dimensions:\n\nThree dimensions, when reading more than one layer (e.g., .read() or .read([1,2])). In such case, the dimensions pattern is (layers, rows, columns)\nTwo dimensions, when reading one specific layer (e.g., .read(1))\n\nLet’s read the first (and only) layer from the srtm.tif raster, using the file connection object src, for example:\n\ns = src.read(1)\ns\n\narray([[1728, 1718, 1715, ..., 2654, 2674, 2685],\n       [1737, 1727, 1717, ..., 2649, 2677, 2693],\n       [1739, 1734, 1727, ..., 2644, 2672, 2695],\n       ...,\n       [1326, 1328, 1329, ..., 1777, 1778, 1775],\n       [1320, 1323, 1326, ..., 1771, 1770, 1772],\n       [1319, 1319, 1322, ..., 1768, 1770, 1772]], dtype=uint16)\n\n\nThe result s is a two-dimensional numpy array.\n\n\n2.3.3 Raster from scratch\nIn this section, we are going to demonstrate creation of rasters from scratch. We are going to create two small rasters, elev and grain, which we are going to use in examples later on in the book. Unlike creating a vector layer, creating a raster from scratch is rarely needed in practive because aligning a raster with the right spatial extent is difficult to do programmatically (GIS software is a better fit for the job). Nevertheless, the examples will be useful to become more familiar with the rasterio data structures.\nA raster is basically an array combined with georeferencing information, namely:\n\nA transformation matrix (linking pixel indices with coordinates)\nA CRS definition\n\nTherefore, to create a raster, we first need to have an array with the values, then supplement it with the georeferencing information. Let’s create the arrays elev and grain. The elev array is a 6 by 6 array with sequential values from 1 to 36. It can be created as follows:\n\nelev = np.arange(1, 37, dtype=np.uint8).reshape(6, 6)\nelev\n\narray([[ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\nThe grain array represents a categorical raster with values 0, 1, 2, corresponding to categories “clay”, “silt”, “sand”, respectively. We will create if from a specific arrangement of pixel values, as follows:\n\nv = [1, 0, 1, 2, 2, 2, 0, 2, 0, 0, 2, 1, 0, 2, 2, 0, 0, 2, 0, 0, 1, 1,\n       1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 0, 2]\ngrain = np.array(v, dtype=np.uint8).reshape(6, 6)\ngrain\n\narray([[1, 0, 1, 2, 2, 2],\n       [0, 2, 0, 0, 2, 1],\n       [0, 2, 2, 0, 0, 2],\n       [0, 0, 1, 1, 1, 1],\n       [1, 1, 1, 2, 1, 1],\n       [2, 1, 2, 2, 0, 2]], dtype=uint8)\n\n\nNote that in both cases we are using the uint8 (unsigned integer in 8 bits, i.e., 0-255) data type, which is minimally sufficient to represent all possible values of the given rasters. This is the recommended approach for a minimal memory footprint.\nWhat is missing is the raster transform (see Section 2.3.2). In this case, since the rasters are arbitrary, we also set up an arbitrary transformation matrix, where:\n\nthe origin (\\(x_{min}\\), \\(y_{max}\\)) is at -1.5,1.5, and\nand resolution (\\(delta_{x}\\), \\(delta_{y}\\)) is 0.5,-0.5.\n\nIn terms of code, we do that as follows, using rasterio.transform.from_origin:\n\nnew_transform = rasterio.transform.from_origin(west=-1.5, north=1.5, xsize=0.5, ysize=0.5)\nnew_transform\n\nAffine(0.5, 0.0, -1.5,\n       0.0, -0.5, 1.5)\n\n\nNote that, confusingly, \\(delta_{y}\\) is defined in rasterio.transform.from_origin using a positive value (0.5), even though it is eventuially negative (-0.5)!\nThe raster can now be plotted in its coordinate system, passing the array along with the transformation matrix to show:\n\nshow(elev, transform=new_transform);\n\n\n\n\nThe grain raster can be plotted the same way, as we are going to use the same transformation matrix for it as well:\n\nshow(grain, transform=new_transform);\n\n\n\n\nAt this point, we can work with the raster using rasterio:\n\nPassing the transformation matrix wherever true raster pixel coordinates are important (such as in function show above)\nKeeping in mind that any other layer we use in the analysis is in the same CRS of those coordinates\n\nFinally, to export the raster for permanent storage, along with the CRS definition, we need to go through the following steps:\n\nCreate a raster file connection (where we set the transform and the CRS, among other settings)\nWrite the array with raster values into the connection\nClose the connection\n\nIn the case of elev, we do it as follows:\nnew_dataset = rasterio.open(\n    'output/elev.tif', 'w', \n    driver = 'GTiff',\n    height = elev.shape[0],\n    width = elev.shape[1],\n    count = 1,\n    dtype = elev.dtype,\n    crs = 4326,\n    transform = new_transform\n)\nnew_dataset.write(elev, 1)\nnew_dataset.close()\nNote that the CRS we (arbitrarily) set for the elev raster is WGS84, defined using crs=4326 according to the EPSG code.\nHere is how we export the grain raster as well, using almost the exact same code just with elev replaced with grain:\nnew_dataset = rasterio.open(\n    'output/grain.tif', 'w', \n    driver = 'GTiff',\n    height = grain.shape[0],\n    width = grain.shape[1],\n    count = 1,\n    dtype = grain.dtype,\n    crs = 4326,\n    transform = new_transform\n)\nnew_dataset.write(grain, 1)\nnew_dataset.close()\nDon’t worry if the raster export code is unclear. We will elaborate on the details of raster output in @read-write.\nAs a result, the files elev.tif and grain.tif are written into the output directory. These are identical to the elev.tif and grain.tif files in the data directory which we use later on in the examples (for example, Section 3.4.1).\nNote that the transform matrices and dimensions of elev and grain are identical. This means that the rasters are overlapping, and can be combined into one two-band raster, combined in raster algebra operations (Section 4.4.2), etc."
  },
  {
    "objectID": "02-spatial-data.html#coordinate-reference-systems",
    "href": "02-spatial-data.html#coordinate-reference-systems",
    "title": "2  Geographic data in Python",
    "section": "2.4 Coordinate Reference Systems",
    "text": "2.4 Coordinate Reference Systems\nVector and raster spatial data types share concepts intrinsic to spatial data. Perhaps the most fundamental of these is the Coordinate Reference System (CRS), which defines how the spatial elements of the data relate to the surface of the Earth (or other bodies). CRSs are either geographic or projected, as introduced at the beginning of this chapter (see …). This section explains each type, laying the foundations for Chapter 7, which provides a deep dive into setting, transforming, and querying CRSs.\n\n2.4.1 Geographic coordinate systems\nGeographic coordinate systems identify any location on the Earth’s surface using two values — longitude and latitude (see left panel of Figure …). Longitude is a location in the East-West direction in angular distance from the Prime Meridian plane, while latitude is an angular distance North or South of the equatorial plane. Distances in geographic CRSs are therefore not measured in meters. This has important consequences, as demonstrated in Section 7.\nA spherical or ellipsoidal surface represents the surface of the Earth in geographic coordinate systems. Spherical models assume that the Earth is a perfect sphere of a given radius — they have the advantage of simplicity, but, at the same time, they are inaccurate: the Earth is not a sphere! Ellipsoidal models are defined by two parameters: the equatorial radius and the polar radius. These are suitable because the Earth is compressed: the equatorial radius is around 11.5 km longer than the polar radius. \nEllipsoids are part of a broader component of CRSs: the datum. This contains information on what ellipsoid to use and the precise relationship between the Cartesian coordinates and location on the Earth’s surface. There are two types of datum — geocentric (such as WGS84) and local (such as NAD83). You can see examples of these two types of datums in Figure …. Black lines represent a geocentric datum, whose center is located in the Earth’s center of gravity and is not optimized for a specific location. In a local datum, shown as a purple dashed line, the ellipsoidal surface is shifted to align with the surface at a particular location. These allow local variations on Earth’s surface, such as large mountain ranges, to be accounted for in a local CRS. \n\n\n2.4.2 Projected coordinate reference systems\nAll projected CRSs are based on a geographic CRS, described in the previous section, and rely on map projections to convert the three-dimensional surface of the Earth into Easting and Northing (x and y) values in a projected CRS. Projected CRSs are based on Cartesian coordinates on an implicitly flat surface.  They have an origin, x and y axes, and a linear unit of measurement such as meters.\nThis transition cannot be done without adding some deformations. Therefore, some properties of the Earth’s surface are distorted in this process, such as area, direction, distance, and shape. A projected coordinate system can preserve only one or two of those properties. Projections are often named based on a property they preserve: equal-area preserves area, azimuthal preserves direction, equidistant preserves distance, and conformal preserves local shape.\nThere are three main groups of projection types: conic, cylindrical, and planar (azimuthal). In a conic projection, the Earth’s surface is projected onto a cone along a single line of tangency or two lines of tangency. Distortions are minimized along the tangency lines and rise with the distance from those lines in this projection. Therefore, it is best suited for maps of mid-latitude areas. A cylindrical projection maps the surface onto a cylinder. This projection could also be created by touching the Earth’s surface along a single line of tangency or two lines of tangency. Cylindrical projections are used most often when mapping the entire world. A planar projection projects data onto a flat surface touching the globe at a point or along a line of tangency. It is typically used in mapping polar regions.\nLike most open-source geospatial software, the geopandas and rasterio packages use the PROJ software for CRS definition and calculations. The pyproj package is a low-level interface to PROJ. Using its functions, we can examine the list of supported projections:\n\nimport pyproj\nepsg_codes = pyproj.get_codes('EPSG', 'CRS')  ## List of supported EPSG codes\nepsg_codes[:5]  ## print first five\n\n['2000', '20000', '20001', '20004', '20005']\n\n\n\npyproj.CRS.from_epsg(4326)  ## Printout of WGS84 CRS (EPSG:4326)\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nA quick summary of different projections, their types, properties, and suitability can be found in “Map Projections” (1993) and at https://www.geo-projections.com/. We will expand on CRSs and explain how to project from one CRS to another in Chapter 7. But, for now, it is sufficient to know:\n\nThat coordinate systems are a key component of geographic objects\nKnowing which CRS your data is in, and whether it is in geographic (lon/lat) or projected (typically meters), is important and has consequences for how Python handles spatial and geometry operations\nCRSs of geopandas (vector layer or geometry column) and rasterio (raster) objects can be queried with the .crs property\n\nHere is a demonstration of the last bullet point:\n\ngdf.crs\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\nsrc.crs\n\nCRS.from_epsg(4326)"
  },
  {
    "objectID": "02-spatial-data.html#units",
    "href": "02-spatial-data.html#units",
    "title": "2  Geographic data in Python",
    "section": "2.5 Units",
    "text": "2.5 Units\nAn essential feature of CRSs is that they contain information about spatial units. Clearly, it is vital to know whether a house’s measurements are in feet or meters, and the same applies to maps. It is a good cartographic practice to add a scale bar or some other distance indicator onto maps to demonstrate the relationship between distances on the page or screen and distances on the ground. Likewise, it is important for the user to be aware of the units in which the geometry coordinates are, to ensure that subsequent calculations are done in the right context.\nPython spatial data structures in geopandas and rasterio do not natively support the concept of measurement. The coordinates of a vector layer or a raster are plain numbers, referring to an arbitrary plane. For example, according to the .transform matrix of srtm.tif we can see that the raster resolution is 0.000833 and that its CRS is WGS84 (EPSG: 4326). We may know (or can find out) that the units of WGS84 are decimal degrees. However, that information is not encoded in any numeric calculation.\n\nsrc.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint16',\n 'nodata': 65535.0,\n 'width': 465,\n 'height': 457,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.0008333333332777796, 0.0, -113.23958321278403,\n        0.0, -0.0008333333332777843, 37.512916763165805)}\n\n\nConsequently, we need to be aware of the CRS units we are working with. Typically, these are decimal degrees, in a geographic CRS, or \\(m\\), in a projected CRS, although there are exceptions. Geometric calculations such as length, area, or distance, return plain numbers in the same units of the CRS (such as \\(m\\) or \\(m^2\\)). It is up to the user to determine which units the result is given in and treat the result accordingly. For example, if the area output was in \\(m^2\\) and we need the result in \\(km^2\\), then we need to divide the result by \\(1000^2\\)."
  },
  {
    "objectID": "02-spatial-data.html#exercises",
    "href": "02-spatial-data.html#exercises",
    "title": "2  Geographic data in Python",
    "section": "2.6 Exercises",
    "text": "2.6 Exercises\n…"
  },
  {
    "objectID": "03-attribute-operations.html",
    "href": "03-attribute-operations.html",
    "title": "3  Attribute data operations",
    "section": "",
    "text": "Packages…\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport rasterio\nSample data…\n\n\nAttempting to get the data\n\n\nworld = gpd.read_file('data/world.gpkg')\nsrc_elev = rasterio.open('data/elev.tif')\nsrc_multi_rast = rasterio.open('data/landsat.tif')"
  },
  {
    "objectID": "03-attribute-operations.html#introduction",
    "href": "03-attribute-operations.html#introduction",
    "title": "3  Attribute data operations",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction\nAttribute data is non-spatial information associated with geographic (geometry) data. A bus stop provides a simple example: its position would typically be represented by latitude and longitude coordinates (geometry data), in addition to its name. The Elephant & Castle / New Kent Road stop in London, for example has coordinates of -0.098 degrees longitude and 51.495 degrees latitude which can be represented as POINT (-0.098 51.495) in the Simple Feature representation described in Chapter 2. Attributes such as the name attribute of the POINT feature (to use Simple Features terminology) are the topic of this chapter.\nAnother example is the elevation value (attribute) for a specific grid cell in raster data. Unlike the vector data model, the raster data model stores the coordinate of the grid cell indirectly, meaning the distinction between attribute and spatial information is less clear. To illustrate the point, think of a pixel in the 3rd row and the 4th column of a raster matrix. Its spatial location is defined by its index in the matrix: move from the origin four cells in the x direction (typically east and right on maps) and three cells in the y direction (typically south and down). The raster’s resolution defines the distance for each x- and y-step which is specified in a header. The header is a vital component of raster datasets which specifies how pixels relate to geographic coordinates (see also Chapter @spatial-operations).\nThis teaches how to manipulate geographic objects based on attributes such as the names of bus stops in a vector dataset and elevations of pixels in a raster dataset. For vector data, this means techniques such as subsetting and aggregation (see Section 3.3.1 and Section 3.3.2). Section 3.3.3 and Section 3.3.4 demonstrate how to join data onto simple feature objects using a shared ID and how to create new variables, respectively. Each of these operations has a spatial equivalent: [ operator for subsetting a (Geo)DataFrame using a boolean Series, for example, is applicable both for subsetting objects based on their attribute and spatial relations derived using methods such as .intersects; you can also join attributes in two geographic datasets using spatial joins. This is good news: skills developed in this chapter are cross-transferable. Chapter 4 extends the methods presented here to the spatial world.\nAfter a deep dive into various types of vector attribute operations in the next section, raster attribute data operations are covered in Section 3.4, which demonstrates how to create raster layers containing continuous and categorical attributes and extracting cell values from one or more layer (raster subsetting). Section 3.4.2 provides an overview of ‘global’ raster operations which can be used to summarize entire raster datasets."
  },
  {
    "objectID": "03-attribute-operations.html#vector-attribute-manipulation",
    "href": "03-attribute-operations.html#vector-attribute-manipulation",
    "title": "3  Attribute data operations",
    "section": "3.3 Vector attribute manipulation",
    "text": "3.3 Vector attribute manipulation\nAs mentioned in Section 2.2.2, vector layers (GeoDataFrame, from package geopandas) are basically extended tables (DataFrame from package pandas), the difference being that a vector layer has a geometry column. Since GeoDataFrame extends DataFrame, all ordinary table-related operations from package pandas are supported for vector layers as well, as shown below.\n\n3.3.1 Vector attribute subsetting\npandas supports several subsetting interfaces, though the most recommended ones are:\n\n.loc, which uses pandas indices, and\n.iloc, which uses (implicit) numpy-style numeric indices.\n\nIn both cases the method is followed by square brackets, and two indices, separated by a comma. Each index can comprise:\n\nA specific value, as in 1\nA slice, as in 0:3\nA list, as in [0,2,4]\n:—indicating “all” indices\n\nThe once exception which we are going to with subsetting by indices is when selecting columns, directly using a list, as in df[['a','b']], instead of df.loc[:, ['a','b']], to select columns 'a' and 'b' from df.\nHere are few examples of subsetting the GeoDataFrame of world countries.\nSubsetting rows by position:\n\nworld.iloc[0:3, :]\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      lifeExp\n      gdpPercap\n      geometry\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Oceania\n      ...\n      69.960\n      8222.253784\n      MULTIPOLYGON (((-180.00000 -16....\n    \n    \n      1\n      TZ\n      Tanzania\n      Africa\n      ...\n      64.163\n      2402.099404\n      MULTIPOLYGON (((33.90371 -0.950...\n    \n    \n      2\n      EH\n      Western Sahara\n      Africa\n      ...\n      NaN\n      NaN\n      MULTIPOLYGON (((-8.66559 27.656...\n    \n  \n\n3 rows × 11 columns\n\n\n\nSubsetting columns by position:\n\nworld.iloc[:, 0:3]\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Oceania\n    \n    \n      1\n      TZ\n      Tanzania\n      Africa\n    \n    \n      2\n      EH\n      Western Sahara\n      Africa\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      XK\n      Kosovo\n      Europe\n    \n    \n      175\n      TT\n      Trinidad and Tobago\n      North America\n    \n    \n      176\n      SS\n      South Sudan\n      Africa\n    \n  \n\n177 rows × 3 columns\n\n\n\nSubsetting rows and columns by position:\n\nworld.iloc[0:3, 0:3]\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Oceania\n    \n    \n      1\n      TZ\n      Tanzania\n      Africa\n    \n    \n      2\n      EH\n      Western Sahara\n      Africa\n    \n  \n\n\n\n\nSubsetting columns by name:\n\nworld[['name_long', 'geometry']]\n\n\n\n\n\n  \n    \n      \n      name_long\n      geometry\n    \n  \n  \n    \n      0\n      Fiji\n      MULTIPOLYGON (((-180.00000 -16....\n    \n    \n      1\n      Tanzania\n      MULTIPOLYGON (((33.90371 -0.950...\n    \n    \n      2\n      Western Sahara\n      MULTIPOLYGON (((-8.66559 27.656...\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      174\n      Kosovo\n      MULTIPOLYGON (((20.59025 41.855...\n    \n    \n      175\n      Trinidad and Tobago\n      MULTIPOLYGON (((-61.68000 10.76...\n    \n    \n      176\n      South Sudan\n      MULTIPOLYGON (((30.83385 3.5091...\n    \n  \n\n177 rows × 2 columns\n\n\n\n“Slice” of columns between given ones:\n\nworld.loc[:, 'name_long':'pop']\n\n\n\n\n\n  \n    \n      \n      name_long\n      continent\n      region_un\n      ...\n      type\n      area_km2\n      pop\n    \n  \n  \n    \n      0\n      Fiji\n      Oceania\n      Oceania\n      ...\n      Sovereign country\n      19289.970733\n      885806.0\n    \n    \n      1\n      Tanzania\n      Africa\n      Africa\n      ...\n      Sovereign country\n      932745.792357\n      52234869.0\n    \n    \n      2\n      Western Sahara\n      Africa\n      Africa\n      ...\n      Indeterminate\n      96270.601041\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      Kosovo\n      Europe\n      Europe\n      ...\n      Sovereign country\n      11230.261672\n      1821800.0\n    \n    \n      175\n      Trinidad and Tobago\n      North America\n      Americas\n      ...\n      Sovereign country\n      7737.809855\n      1354493.0\n    \n    \n      176\n      South Sudan\n      Africa\n      Africa\n      ...\n      Sovereign country\n      624909.099086\n      11530971.0\n    \n  \n\n177 rows × 7 columns\n\n\n\nSubsetting by a boolean series:\n\nx = np.array([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0], dtype=bool)\nworld.iloc[:, x]\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      pop\n      lifeExp\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      885806.0\n      69.960000\n    \n    \n      1\n      TZ\n      Tanzania\n      52234869.0\n      64.163000\n    \n    \n      2\n      EH\n      Western Sahara\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      XK\n      Kosovo\n      1821800.0\n      71.097561\n    \n    \n      175\n      TT\n      Trinidad and Tobago\n      1354493.0\n      70.426000\n    \n    \n      176\n      SS\n      South Sudan\n      11530971.0\n      55.817000\n    \n  \n\n177 rows × 4 columns\n\n\n\nWe can remove a specific row by id use the .drop method:\n\nworld.drop([2, 3, 5])\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      lifeExp\n      gdpPercap\n      geometry\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Oceania\n      ...\n      69.960000\n      8222.253784\n      MULTIPOLYGON (((-180.00000 -16....\n    \n    \n      1\n      TZ\n      Tanzania\n      Africa\n      ...\n      64.163000\n      2402.099404\n      MULTIPOLYGON (((33.90371 -0.950...\n    \n    \n      4\n      US\n      United States\n      North America\n      ...\n      78.841463\n      51921.984639\n      MULTIPOLYGON (((-171.73166 63.7...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      XK\n      Kosovo\n      Europe\n      ...\n      71.097561\n      8698.291559\n      MULTIPOLYGON (((20.59025 41.855...\n    \n    \n      175\n      TT\n      Trinidad and Tobago\n      North America\n      ...\n      70.426000\n      31181.821196\n      MULTIPOLYGON (((-61.68000 10.76...\n    \n    \n      176\n      SS\n      South Sudan\n      Africa\n      ...\n      55.817000\n      1935.879400\n      MULTIPOLYGON (((30.83385 3.5091...\n    \n  \n\n174 rows × 11 columns\n\n\n\nOr remove specific columns using the .drop method and axis=1 (i.e., columns):\n\nworld.drop(['name_long', 'continent'], axis=1)\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      region_un\n      subregion\n      ...\n      lifeExp\n      gdpPercap\n      geometry\n    \n  \n  \n    \n      0\n      FJ\n      Oceania\n      Melanesia\n      ...\n      69.960000\n      8222.253784\n      MULTIPOLYGON (((-180.00000 -16....\n    \n    \n      1\n      TZ\n      Africa\n      Eastern Africa\n      ...\n      64.163000\n      2402.099404\n      MULTIPOLYGON (((33.90371 -0.950...\n    \n    \n      2\n      EH\n      Africa\n      Northern Africa\n      ...\n      NaN\n      NaN\n      MULTIPOLYGON (((-8.66559 27.656...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      XK\n      Europe\n      Southern Europe\n      ...\n      71.097561\n      8698.291559\n      MULTIPOLYGON (((20.59025 41.855...\n    \n    \n      175\n      TT\n      Americas\n      Caribbean\n      ...\n      70.426000\n      31181.821196\n      MULTIPOLYGON (((-61.68000 10.76...\n    \n    \n      176\n      SS\n      Africa\n      Eastern Africa\n      ...\n      55.817000\n      1935.879400\n      MULTIPOLYGON (((30.83385 3.5091...\n    \n  \n\n177 rows × 9 columns\n\n\n\nWe can rename (some of) the selected columns using the .rename method:\n\nworld[['name_long', 'pop']].rename(columns={'pop': 'population'})\n\n\n\n\n\n  \n    \n      \n      name_long\n      population\n    \n  \n  \n    \n      0\n      Fiji\n      885806.0\n    \n    \n      1\n      Tanzania\n      52234869.0\n    \n    \n      2\n      Western Sahara\n      NaN\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      174\n      Kosovo\n      1821800.0\n    \n    \n      175\n      Trinidad and Tobago\n      1354493.0\n    \n    \n      176\n      South Sudan\n      11530971.0\n    \n  \n\n177 rows × 2 columns\n\n\n\nThe standard numpy comparison operators can be used in boolean subsetting, as illustrated in Table Table 3.1.\n\n\nTable 3.1: Comparison operators that return Booleans (True/False).\n\n\nSymbol\nName\n\n\n\n\n==\nEqual to\n\n\n!=\nNot equal to\n\n\n>, <\nGreater/Less than\n\n\n>=, <=\nGreater/Less than or equal\n\n\n&, |, ~\nLogical operators: And, Or, Not\n\n\n\n\nA demonstration of the utility of using logical vectors for subsetting is shown in the code chunk below. This creates a new object, small_countries, containing nations whose surface area is smaller than 10,000 km2:\n\nidx_small = world['area_km2'] < 10000  ## a logical 'Series'\nsmall_countries = world[idx_small]\nsmall_countries\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      lifeExp\n      gdpPercap\n      geometry\n    \n  \n  \n    \n      45\n      PR\n      Puerto Rico\n      North America\n      ...\n      79.390122\n      35066.046376\n      MULTIPOLYGON (((-66.28243 18.51...\n    \n    \n      79\n      PS\n      Palestine\n      Asia\n      ...\n      73.126000\n      4319.528283\n      MULTIPOLYGON (((35.39756 31.489...\n    \n    \n      89\n      VU\n      Vanuatu\n      Oceania\n      ...\n      71.709000\n      2892.341604\n      MULTIPOLYGON (((166.79316 -15.6...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      160\n      None\n      Northern Cyprus\n      Asia\n      ...\n      NaN\n      NaN\n      MULTIPOLYGON (((32.73178 35.140...\n    \n    \n      161\n      CY\n      Cyprus\n      Asia\n      ...\n      80.173000\n      29786.365653\n      MULTIPOLYGON (((32.73178 35.140...\n    \n    \n      175\n      TT\n      Trinidad and Tobago\n      North America\n      ...\n      70.426000\n      31181.821196\n      MULTIPOLYGON (((-61.68000 10.76...\n    \n  \n\n7 rows × 11 columns\n\n\n\nThe intermediary idx_small (short for index representing small countries) is a boolean Series that can be used to subset the seven smallest countries in the world by surface area. A more concise command, which omits the intermediary object, generates the same result:\nsmall_countries = world[world['area_km2'] < 10000]\nThe various methods shown above can be chained for any combination with several subsetting steps. For example:\n\nworld[world['continent'] == 'Asia']  \\\n    .loc[:, ['name_long', 'continent']]  \\\n    .iloc[0:5, :]\n\n\n\n\n\n  \n    \n      \n      name_long\n      continent\n    \n  \n  \n    \n      5\n      Kazakhstan\n      Asia\n    \n    \n      6\n      Uzbekistan\n      Asia\n    \n    \n      8\n      Indonesia\n      Asia\n    \n    \n      24\n      Timor-Leste\n      Asia\n    \n    \n      76\n      Israel\n      Asia\n    \n  \n\n\n\n\nWe can also combine indexes\n\nidx_small = world['area_km2'] < 10000\nidx_asia = world['continent'] == 'Asia'\nworld.loc[idx_small & idx_asia, ['name_long', 'continent', 'area_km2']]\n\n\n\n\n\n  \n    \n      \n      name_long\n      continent\n      area_km2\n    \n  \n  \n    \n      79\n      Palestine\n      Asia\n      5037.103826\n    \n    \n      160\n      Northern Cyprus\n      Asia\n      3786.364506\n    \n    \n      161\n      Cyprus\n      Asia\n      6207.006191\n    \n  \n\n\n\n\n\n\n3.3.2 Vector attribute aggregation\nAggregation involves summarizing data with one or more grouping variables, typically from columns in the table to be aggregated (geographic aggregation is covered in the next chapter). An example of attribute aggregation is calculating the number of people per continent based on country-level data (one row per country). The world dataset contains the necessary ingredients: the columns pop and continent, the population and the grouping variable, respectively. The aim is to find the sum() of country populations for each continent, resulting in a smaller data frame (aggregation is a form of data reduction and can be a useful early step when working with large datasets). This can be done with a combination of .groupby and .sum:\n\nworld_agg1 = world[['continent', 'pop']].groupby('continent').sum()\nworld_agg1\n\n\n\n\n\n  \n    \n      \n      pop\n    \n    \n      continent\n      \n    \n  \n  \n    \n      Africa\n      1.154947e+09\n    \n    \n      Antarctica\n      0.000000e+00\n    \n    \n      Asia\n      4.311408e+09\n    \n    \n      ...\n      ...\n    \n    \n      Oceania\n      3.775783e+07\n    \n    \n      Seven seas (open ocean)\n      0.000000e+00\n    \n    \n      South America\n      4.120608e+08\n    \n  \n\n8 rows × 1 columns\n\n\n\nThe result is a (non-spatial) table with eight rows, one per continent, and two columns reporting the name and population of each continent.\nAlternatively, to include the geometry in the aggregation result, we can use the .dissolve method. That way, in addition to the summed population we also get the associated geometry per continent, i.e., the union of all countries. Note that we use the by parameter to choose which column(s) are used for grouping, and the aggfunc parameter to choose the summary function for non-geometry columns:\n\nworld_agg2 = world[['continent', 'pop', 'geometry']] \\\n    .dissolve(by='continent', aggfunc='sum') \\\n    .reset_index()\nworld_agg2\n\n\n\n\n\n  \n    \n      \n      continent\n      geometry\n      pop\n    \n  \n  \n    \n      0\n      Africa\n      MULTIPOLYGON (((-11.43878 6.785...\n      1.154947e+09\n    \n    \n      1\n      Antarctica\n      MULTIPOLYGON (((-61.13898 -79.9...\n      0.000000e+00\n    \n    \n      2\n      Asia\n      MULTIPOLYGON (((48.67923 14.003...\n      4.311408e+09\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      5\n      Oceania\n      MULTIPOLYGON (((147.91405 -43.2...\n      3.775783e+07\n    \n    \n      6\n      Seven seas (open ocean)\n      POLYGON ((68.93500 -48.62500, 6...\n      0.000000e+00\n    \n    \n      7\n      South America\n      MULTIPOLYGON (((-68.63999 -55.5...\n      4.120608e+08\n    \n  \n\n8 rows × 3 columns\n\n\n\nFigure 3.1 shows the result:\n\nworld_agg2.plot(column='pop', legend=True);\n\n\n\n\nFigure 3.1: Continents with summed population\n\n\n\n\nThe resulting world_agg2 object is a vector layer containing 8 features representing the continents of the world (and the open ocean).\nOther options for the aggfunc parameter in .dissolve include:\n\n'first'\n'last'\n'min'\n'max'\n'sum'\n'mean'\n'median'\n\nAdditionally, we can pass a custom function.\nas a more complicated example, here is how we can calculate the summed population, summed area, and count of countries, per continent. We calculate the area_km2+pop and the name_long (renamed to n) columns in two separate expressions (since we are using two different functions), then join the results. Also note that only one of the expression is a spatial aggregation, since the continent geometries are identical there is no need to calculate them twice:\n\nworld_agg3a = world[['continent', 'area_km2', 'pop', 'geometry']] \\\n    .dissolve(by='continent', aggfunc='sum')\nworld_agg3b = world[['continent', 'name_long']] \\\n    .groupby('continent') \\\n    .nunique() \\\n    .reset_index() \\\n    .rename(columns={'name_long': 'n'})\nworld_agg3 = pd.merge(world_agg3a, world_agg3b, on='continent')\nworld_agg3\n\n\n\n\n\n  \n    \n      \n      continent\n      geometry\n      area_km2\n      pop\n      n\n    \n  \n  \n    \n      0\n      Africa\n      MULTIPOLYGON (((-11.43878 6.785...\n      2.994620e+07\n      1.154947e+09\n      51\n    \n    \n      1\n      Antarctica\n      MULTIPOLYGON (((-61.13898 -79.9...\n      1.233596e+07\n      0.000000e+00\n      1\n    \n    \n      2\n      Asia\n      MULTIPOLYGON (((48.67923 14.003...\n      3.125246e+07\n      4.311408e+09\n      47\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      5\n      Oceania\n      MULTIPOLYGON (((147.91405 -43.2...\n      8.504489e+06\n      3.775783e+07\n      7\n    \n    \n      6\n      Seven seas (open ocean)\n      POLYGON ((68.93500 -48.62500, 6...\n      1.160257e+04\n      0.000000e+00\n      1\n    \n    \n      7\n      South America\n      MULTIPOLYGON (((-68.63999 -55.5...\n      1.776259e+07\n      4.120608e+08\n      13\n    \n  \n\n8 rows × 5 columns\n\n\n\nFigure Figure 3.2 visualizes the resulting layer (world_agg3) of continents with the three aggregated attributes.\n\nfig, axes = plt.subplots(ncols=3, figsize=(9,5))\nworld_agg3.plot(column='pop', edgecolor='black', legend=True, ax=axes[0])\nworld_agg3.plot(column='area_km2', edgecolor='black', legend=True, ax=axes[1])\nworld_agg3.plot(column='n', edgecolor='black', legend=True, ax=axes[2])\naxes[0].set_title('Summed population')\naxes[1].set_title('Summed area')\naxes[2].set_title('Count of countries');\n\n\n\n\nFigure 3.2: Continent properties, calculated using spatial aggregation using different functions\n\n\n\n\nLet’s proceed with the last result to demonstrate other table-related operations. Given the world_agg3 continent summary (Figure 3.2), we:\n\ndrop the geometry columns,\ncalculate population density of each continent,\narrange continents by the number countries they contain, and\nkeep only the 3 most populous continents.\n\n\nworld_agg4 = world_agg3.drop('geometry', axis=1)\nworld_agg4['density'] = world_agg4['pop'] / world_agg4['area_km2']\nworld_agg4 = world_agg4.sort_values(by='n', ascending=False)\nworld_agg4 = world_agg4.iloc[:3, :]\nworld_agg4\n\n\n\n\n\n  \n    \n      \n      continent\n      area_km2\n      pop\n      n\n      density\n    \n  \n  \n    \n      0\n      Africa\n      2.994620e+07\n      1.154947e+09\n      51\n      38.567388\n    \n    \n      2\n      Asia\n      3.125246e+07\n      4.311408e+09\n      47\n      137.954201\n    \n    \n      3\n      Europe\n      2.306522e+07\n      6.690363e+08\n      39\n      29.006283\n    \n  \n\n\n\n\n\n\n3.3.3 Vector attribute joining\nCombining data from different sources is a common task in data preparation. Joins do this by combining tables based on a shared ‘key’ variable. pandas has a function named pd.merge for joining tables or vector layers based on common column(s). The pd.merge function follows conventions used in the database language SQL (Grolemund and Wickham 2016). Using pd.merge to join non-spatial datasets to vector layers is the focus of this section. The pd.merge function works the same on tables (DataFrame) and vector layer (GeoDataFrame) objects, the only important difference being the presence of the geometry column (which is not involved in the join operation anyway). The result of data joins can be either a DataFrame or a GeoDataFrame object, depending on the inputs. The most common type of attribute join on spatial data takes a GeoDataFrame object as the first argument and adds columns to it from a DataFrame specified as the second argument.\nTo demonstrate joins, we will combine data on coffee production with the world dataset. The coffee data is in a DataFrame called coffee_data imported from a CSV file. It has 3 columns:\n\nname_long names major coffee-producing nations\ncoffee_production_2016 and coffee_production_2017 contain estimated values for coffee production in units of 60-kg bags in each year.\n\n\ncoffee_data = pd.read_csv('data/coffee_data.csv')\ncoffee_data\n\n\n\n\n\n  \n    \n      \n      name_long\n      coffee_production_2016\n      coffee_production_2017\n    \n  \n  \n    \n      0\n      Angola\n      NaN\n      NaN\n    \n    \n      1\n      Bolivia\n      3.0\n      4.0\n    \n    \n      2\n      Brazil\n      3277.0\n      2786.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      44\n      Zambia\n      3.0\n      NaN\n    \n    \n      45\n      Zimbabwe\n      1.0\n      1.0\n    \n    \n      46\n      Others\n      23.0\n      26.0\n    \n  \n\n47 rows × 3 columns\n\n\n\nA left join, which preserves the first dataset, merges world with coffee_data, based on the common 'name_long' column:\n\nworld_coffee = pd.merge(world, coffee_data, on='name_long', how='left')\nworld_coffee\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      geometry\n      coffee_production_2016\n      coffee_production_2017\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Oceania\n      ...\n      MULTIPOLYGON (((-180.00000 -16....\n      NaN\n      NaN\n    \n    \n      1\n      TZ\n      Tanzania\n      Africa\n      ...\n      MULTIPOLYGON (((33.90371 -0.950...\n      81.0\n      66.0\n    \n    \n      2\n      EH\n      Western Sahara\n      Africa\n      ...\n      MULTIPOLYGON (((-8.66559 27.656...\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      XK\n      Kosovo\n      Europe\n      ...\n      MULTIPOLYGON (((20.59025 41.855...\n      NaN\n      NaN\n    \n    \n      175\n      TT\n      Trinidad and Tobago\n      North America\n      ...\n      MULTIPOLYGON (((-61.68000 10.76...\n      NaN\n      NaN\n    \n    \n      176\n      SS\n      South Sudan\n      Africa\n      ...\n      MULTIPOLYGON (((30.83385 3.5091...\n      NaN\n      NaN\n    \n  \n\n177 rows × 13 columns\n\n\n\nThe result is a GeoDataFrame object identical to the original world object, but with two new variables (coffee_production_2016 and coffee_production_2017) on coffee production. This can be plotted as a map, as illustrated in Figure 3.3:\n\nbase = world.plot(color='white', edgecolor='lightgrey')\nworld_coffee.plot(ax=base, column='coffee_production_2017');\n\n\n\n\nFigure 3.3: World coffee production, thousand 60-kg bags by country, in 2017 (source: International Coffee Organization).\n\n\n\n\nFor joining to work, a ‘key variable’ must be supplied in both datasets. In this case, both world_coffee and world objects contained a variable called name_long. By default pd.merge uses all variables with matching names. However, it is recommended to explicitly specify the names of the columns to be used for matching, like we did in the last example.\nIn case where variable names are not the same, you need to rename (using .rename) them so that it is, then proceed with the join.\nNote that the result world_coffee has the same number of rows as the original dataset world. Although there are only 47 rows of data in coffee_data, all 177 country records are kept intact in world_coffee: rows in the original dataset with no match are assigned np.nan values for the new coffee production variables. This is characteristic of a lift join (specified with how='left') and is what we typically want to do.\nWhat if we only want to keep countries that have a match in the key variable? In that case an inner join can be used:\n\n\n3.3.4 Creating attributes and removing spatial information\nCalculate new column…\nworld2 = world.copy()\nworld2['pop_dens'] = world2['pop'] / world2['area_km2']\nUnite columns…\nworld2['con_reg'] = world['continent'] + ':' + world2['region_un']\nworld2 = world2.drop(['continent', 'region_un'], axis=1)\nSplit column…\nworld2[['continent', 'region_un']] = world2['con_reg'] \\\n    .str.split(':', expand=True)\nRename…\n\nworld2.rename(columns={'name_long': 'name'})\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name\n      subregion\n      ...\n      con_reg\n      continent\n      region_un\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Melanesia\n      ...\n      Oceania:Oceania\n      Oceania\n      Oceania\n    \n    \n      1\n      TZ\n      Tanzania\n      Eastern Africa\n      ...\n      Africa:Africa\n      Africa\n      Africa\n    \n    \n      2\n      EH\n      Western Sahara\n      Northern Africa\n      ...\n      Africa:Africa\n      Africa\n      Africa\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      XK\n      Kosovo\n      Southern Europe\n      ...\n      Europe:Europe\n      Europe\n      Europe\n    \n    \n      175\n      TT\n      Trinidad and Tobago\n      Caribbean\n      ...\n      North America:Americas\n      North America\n      Americas\n    \n    \n      176\n      SS\n      South Sudan\n      Eastern Africa\n      ...\n      Africa:Africa\n      Africa\n      Africa\n    \n  \n\n177 rows × 13 columns\n\n\n\nRenaming all columns…\nnew_names =['i', 'n', 'c', 'r', 's', 't', 'a', 'p', 'l', 'gP', 'geom']\nworld.columns = new_names\nReordering columns, for example reverse alphabetical order…\n\nnames = sorted(world.columns, reverse=True)\nworld2 = world[names]\nworld2\n\n\n\n\n\n  \n    \n      \n      t\n      s\n      r\n      ...\n      gP\n      c\n      a\n    \n  \n  \n    \n      0\n      Sovereign country\n      Melanesia\n      Oceania\n      ...\n      8222.253784\n      Oceania\n      19289.970733\n    \n    \n      1\n      Sovereign country\n      Eastern Africa\n      Africa\n      ...\n      2402.099404\n      Africa\n      932745.792357\n    \n    \n      2\n      Indeterminate\n      Northern Africa\n      Africa\n      ...\n      NaN\n      Africa\n      96270.601041\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      Sovereign country\n      Southern Europe\n      Europe\n      ...\n      8698.291559\n      Europe\n      11230.261672\n    \n    \n      175\n      Sovereign country\n      Caribbean\n      Americas\n      ...\n      31181.821196\n      North America\n      7737.809855\n    \n    \n      176\n      Sovereign country\n      Eastern Africa\n      Africa\n      ...\n      1935.879400\n      Africa\n      624909.099086\n    \n  \n\n177 rows × 11 columns\n\n\n\nDropping geometry…\n\npd.DataFrame(world.drop(columns='geom'))\n\n\n\n\n\n  \n    \n      \n      i\n      n\n      c\n      ...\n      p\n      l\n      gP\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Oceania\n      ...\n      885806.0\n      69.960000\n      8222.253784\n    \n    \n      1\n      TZ\n      Tanzania\n      Africa\n      ...\n      52234869.0\n      64.163000\n      2402.099404\n    \n    \n      2\n      EH\n      Western Sahara\n      Africa\n      ...\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      XK\n      Kosovo\n      Europe\n      ...\n      1821800.0\n      71.097561\n      8698.291559\n    \n    \n      175\n      TT\n      Trinidad and Tobago\n      North America\n      ...\n      1354493.0\n      70.426000\n      31181.821196\n    \n    \n      176\n      SS\n      South Sudan\n      Africa\n      ...\n      11530971.0\n      55.817000\n      1935.879400\n    \n  \n\n177 rows × 10 columns"
  },
  {
    "objectID": "03-attribute-operations.html#sec-manipulating-raster-objects",
    "href": "03-attribute-operations.html#sec-manipulating-raster-objects",
    "title": "3  Attribute data operations",
    "section": "3.4 Manipulating raster objects",
    "text": "3.4 Manipulating raster objects\n\n3.4.1 Raster subsetting\nWhen using rasterio, raster values are accessible through a numpy array, which can be imported with the .read method:\n\nelev = src_elev.read(1)\nelev\n\narray([[ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\nThen, we can access any subset of cell values using numpy methods. For example:\n\nelev[0, 0]  ## Value at row 1, column 1\n\n1\n\n\nCell values can be modified by overwriting existing values in conjunction with a subsetting operation. The following expression, for example, sets the upper left cell of elev to 0:\n\nelev[0, 0] = 0\nelev\n\narray([[ 0,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\nMultiple cells can also be modified in this way:\n\nelev[0, 0:2] = 0\nelev\n\narray([[ 0,  0,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\n\n\n3.4.2 Summarizing raster objects\nGlobal summaries of raster values can be calculated by applying numpy summary functions—such as np.mean—on the array with raster values. For example:\n\nnp.mean(elev)\n\n18.416666666666668\n\n\nNote that “No Data”-safe functions–such as np.nanmean—should be used in case the raster contains “No Data” values which need to be ignored. Before we can demontrate that, we must convert the array from int to float, as int arrays cannot contain np.nan (due to computer memory limitations):\n\nelev1 = elev.copy()\nelev1 = elev1.astype('float64')\nelev1\n\narray([[ 0.,  0.,  3.,  4.,  5.,  6.],\n       [ 7.,  8.,  9., 10., 11., 12.],\n       [13., 14., 15., 16., 17., 18.],\n       [19., 20., 21., 22., 23., 24.],\n       [25., 26., 27., 28., 29., 30.],\n       [31., 32., 33., 34., 35., 36.]])\n\n\nNow we can insert an np.nan value into the array (trying to do so in the original elev array raises an error, try it to see for yourself):\n\nelev1[0, 2] = np.nan\nelev1\n\narray([[ 0.,  0., nan,  4.,  5.,  6.],\n       [ 7.,  8.,  9., 10., 11., 12.],\n       [13., 14., 15., 16., 17., 18.],\n       [19., 20., 21., 22., 23., 24.],\n       [25., 26., 27., 28., 29., 30.],\n       [31., 32., 33., 34., 35., 36.]])\n\n\nHere is a demonstration of calculating a global summary, including the “No Data” value (in which case the summary value is unknown):\n\nnp.mean(elev1)\n\nnan\n\n\nand ignoring the “No Data” value(s):\n\nnp.nanmean(elev1)\n\n18.857142857142858\n\n\nRaster value statistics can be visualized in a variety of ways. One approach is to “flatten” the raster values into a one-dimensional array, then use a graphical function such as plt.hist or plt.boxplot (from matplotlib.pyplot). For example:\n\nx = elev.flatten()\nplt.hist(x);"
  },
  {
    "objectID": "03-attribute-operations.html#exercises",
    "href": "03-attribute-operations.html#exercises",
    "title": "3  Attribute data operations",
    "section": "3.5 Exercises",
    "text": "3.5 Exercises"
  },
  {
    "objectID": "04-spatial-operations.html",
    "href": "04-spatial-operations.html",
    "title": "4  Spatial data operations",
    "section": "",
    "text": "Packages…\nimport geopandas as gpd\nimport numpy as np\nimport os\nimport rasterio\nimport scipy.ndimage\n\nfrom rasterio.plot import show\nLet us load the sample data for this chapter:\nnz = gpd.read_file(\"data/nz.gpkg\")\nnz_height = gpd.read_file(\"data/nz_height.gpkg\")\nworld = gpd.read_file('data/world.gpkg')\nsrc_elev = rasterio.open(\"data/elev.tif\")\nsrc_multi_rast = rasterio.open(\"data/landsat.tif\")\nsrc_grain = rasterio.open('data/grain.tif')"
  },
  {
    "objectID": "04-spatial-operations.html#introduction",
    "href": "04-spatial-operations.html#introduction",
    "title": "4  Spatial data operations",
    "section": "4.2 Introduction",
    "text": "4.2 Introduction"
  },
  {
    "objectID": "04-spatial-operations.html#spatial-vec",
    "href": "04-spatial-operations.html#spatial-vec",
    "title": "4  Spatial data operations",
    "section": "4.3 Spatial operations on vector data",
    "text": "4.3 Spatial operations on vector data\n\n4.3.1 Spatial subsetting\nSpatial subsetting is the process of taking a spatial object and returning a new object containing only features that relate in space to another object. Analogous to attribute subsetting (covered in Section 3.3.1), subsets of GeoDataFrames can be created with square bracket ([) operator using the syntax x[y], where x is an GeoDataFrame from which a subset of rows/features will be returned, and y is the ‘subsetting object’. y, in turn, can be created using one of the binary geometry relation methods, such as .intersects (see Section 4.3.2).\nTo demonstrate spatial subsetting, we will use the nz and nz_height layers, which contain geographic data on the 16 main regions and 101 highest points in New Zealand, respectively (Figure 4.1), in a projected coordinate system. The following code chunk creates an object representing Canterbury, then uses spatial subsetting to return all high points in the region:\ncanterbury = nz[nz[\"Name\"] == \"Canterbury\"]\nsel = nz_height.intersects(canterbury['geometry'].iloc[0])\ncanterbury_height = nz_height[sel]\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\nbase = nz.plot(color=\"white\", edgecolor=\"lightgrey\", ax=axes[0])\nnz_height.plot(ax=base, color=\"None\", edgecolor=\"red\")\nbase = nz.plot(color=\"white\", edgecolor=\"lightgrey\", ax=axes[1])\ncanterbury_height.plot(ax=base, color=\"None\", edgecolor=\"red\");\n\n\n\n\nFigure 4.1: Spatial subsetting of points by intersection with polygon\n\n\n\n\nSpatial subsetting 2…\nnon_canterbury_height = nz_height.overlay(canterbury, how='difference')\nPlot…\n\nbase = nz.plot(color=\"white\", edgecolor=\"lightgrey\")\nnon_canterbury_height.plot(ax=base, color=\"None\", edgecolor=\"red\");\n\n\n\n\n…\n\n\n4.3.2 Topological relations\n…\n\nfrom shapely.geometry import Point, LineString, Polygon\npoints = gpd.GeoSeries([Point(0.2,0.1), Point(0.7,0.2), Point(0.4,0.8)])\nline = LineString([(0.4,0.2), (1,0.5)])\npoly = Polygon([(0,0), (0,1), (1,1), (1,0.5), (0,0)])\n\np = gpd.GeoSeries(poly).plot(color='yellow')\np = gpd.GeoSeries(line).plot(ax=p, color='red')\npoints.plot(ax=p)\n\n<AxesSubplot: >\n\n\n\n\n\n\npoints.intersects(poly)\n\n0     True\n1    False\n2     True\ndtype: bool\n\n\n\npoints.within(poly)\n\n0    False\n1    False\n2     True\ndtype: bool\n\n\n\npoints.touches(poly)\n\n0     True\n1    False\n2    False\ndtype: bool\n\n\n\npoints.disjoint(poly)\n\n0    False\n1     True\n2    False\ndtype: bool\n\n\n\npoints.distance(poly) < 0.2\n\n0    True\n1    True\n2    True\ndtype: bool\n\n\n\n\n4.3.3 DE-9IM strings\n…\n\n\n4.3.4 Spatial joining\nJoining two non-spatial datasets relies on a shared ‘key’ variable, as described in Section 3.3.3. Spatial data joining applies the same concept, but instead relies on spatial relations, described in the previous section. As with attribute data, joining adds new columns to the target object (the argument x in joining functions), from a source object (y).\nThe process is illustrated by the following example: imagine you have ten points randomly distributed across the Earth’s surface and you ask, for the points that are on land, which countries are they in? Implementing this idea in a reproducible example will build your geographic data handling skills and show how spatial joins work. The starting point is to create points that are randomly scattered over the Earth’s surface:\n\nnp.random.seed(11)  ## set seed for reproducibility\nbb = world.total_bounds  ## the world's bounds\nx = np.random.uniform(low=bb[0], high=bb[2], size=10)\ny = np.random.uniform(low=bb[1], high=bb[3], size=10)\nrandom_points = gpd.points_from_xy(x, y, crs=4326)\nrandom_points = gpd.GeoSeries(random_points)\nrandom_points = gpd.GeoDataFrame({'geometry': random_points})\nrandom_points\n\n\n\n\n\n  \n    \n      \n      geometry\n    \n  \n  \n    \n      0\n      POINT (-115.10291 36.78178)\n    \n    \n      1\n      POINT (-172.98891 -71.02938)\n    \n    \n      2\n      POINT (-13.24134 65.23272)\n    \n    \n      3\n      POINT (80.97621 58.85495)\n    \n    \n      4\n      POINT (-28.72671 -61.25002)\n    \n    \n      5\n      POINT (-5.24625 19.83849)\n    \n    \n      6\n      POINT (-175.39891 -86.34517)\n    \n    \n      7\n      POINT (-4.54623 -69.64082)\n    \n    \n      8\n      POINT (159.05039 -34.99599)\n    \n    \n      9\n      POINT (126.28622 -62.49509)\n    \n  \n\n\n\n\nThe scenario illustrated in Figure 4.2 shows that the random_points object (top left) lacks attribute data, while the world (top right) has attributes, including country names shown for a sample of countries in the legend. Spatial joins are implemented with gpd.sjoin, as illustrated in the code chunk below. The output is the random_joined object which is illustrated in Figure 4.2 (bottom left). Before creating the joined dataset, we use spatial subsetting to create world_random, which contains only countries that contain random points, to verify the number of country names returned in the joined dataset should be four (see the top right panel of Figure 4.2).\n\n# Subset\nworld_random = world[world.intersects(random_points.unary_union)]\nworld_random\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      lifeExp\n      gdpPercap\n      geometry\n    \n  \n  \n    \n      4\n      US\n      United States\n      North America\n      ...\n      78.841463\n      51921.984639\n      MULTIPOLYGON (((-171.73166 63.7...\n    \n    \n      18\n      RU\n      Russian Federation\n      Europe\n      ...\n      70.743659\n      25284.586202\n      MULTIPOLYGON (((-180.00000 64.9...\n    \n    \n      52\n      ML\n      Mali\n      Africa\n      ...\n      57.007000\n      1865.160622\n      MULTIPOLYGON (((-11.51394 12.44...\n    \n    \n      159\n      AQ\n      Antarctica\n      Antarctica\n      ...\n      NaN\n      NaN\n      MULTIPOLYGON (((-180.00000 -89....\n    \n  \n\n4 rows × 11 columns\n\n\n\n\n# Spatial join\nrandom_joined = gpd.sjoin(random_points, world, how='left')\nrandom_joined\n\n\n\n\n\n  \n    \n      \n      geometry\n      index_right\n      iso_a2\n      ...\n      pop\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      0\n      POINT (-115.10291 36.78178)\n      4.0\n      US\n      ...\n      318622525.0\n      78.841463\n      51921.984639\n    \n    \n      1\n      POINT (-172.98891 -71.02938)\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      POINT (-13.24134 65.23272)\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      POINT (80.97621 58.85495)\n      18.0\n      RU\n      ...\n      143819666.0\n      70.743659\n      25284.586202\n    \n    \n      4\n      POINT (-28.72671 -61.25002)\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n    \n    \n      5\n      POINT (-5.24625 19.83849)\n      52.0\n      ML\n      ...\n      16962846.0\n      57.007000\n      1865.160622\n    \n    \n      6\n      POINT (-175.39891 -86.34517)\n      159.0\n      AQ\n      ...\n      NaN\n      NaN\n      NaN\n    \n    \n      7\n      POINT (-4.54623 -69.64082)\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n    \n    \n      8\n      POINT (159.05039 -34.99599)\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n    \n    \n      9\n      POINT (126.28622 -62.49509)\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n    \n  \n\n10 rows × 12 columns\n\n\n\n\nfig, axes = plt.subplots(2, 2, figsize=(9, 5))\nbase = world.plot(color=\"white\", edgecolor=\"lightgrey\", ax=axes[0][0])\nrandom_points.plot(ax=base, color=\"None\", edgecolor=\"red\")\nbase = world.plot(color=\"white\", edgecolor=\"lightgrey\", ax=axes[0][1])\nworld_random.plot(ax=base, column='name_long')\nbase = world.plot(color=\"white\", edgecolor=\"lightgrey\", ax=axes[1][0])\nrandom_joined.geometry.plot(ax=base, color='grey');\nrandom_joined.plot(ax=base, column='name_long', legend=True);\n\n\n\n\nFigure 4.2: Illustration of a spatial join. A new attribute variable is added to random points (top left) from source world object (top right) resulting in the data represented in the final panel.\n\n\n\n\n\n\n4.3.5 Non-overlapping joins\n…\n\n\n4.3.6 Spatial aggregation\nAs with attribute data aggregation, spatial data aggregation condenses data: aggregated outputs have fewer rows than non-aggregated inputs. Statistical aggregating functions, such as mean, average, or sum, summarise multiple values of a variable, and return a single value per grouping variable. Section … demonstrated how the .groupby method, combined with summary functions such as .sum, condense data based on attribute variables. This section shows how grouping by spatial objects can be acheived using spatial joins combined with non-spatial aggregation.\nReturning to the example of New Zealand, imagine you want to find out the average height of high points in each region: it is the geometry of the source (nz, in this case) that defines how values in the target object (nz_height) are grouped. This can be done in two steps. First, we “attach” the region classification of each point, using spatial join. Note that just one distinct attribute (such as Name in this case) is sufficient for our purposes:\n\ngpd.sjoin(nz_height, nz[['Name', 'geometry']], how='left')\n\n\n\n\n\n  \n    \n      \n      t50_fid\n      elevation\n      geometry\n      index_right\n      Name\n    \n  \n  \n    \n      0\n      2353944\n      2723\n      POINT (1204142.603 5049971.287)\n      12\n      Southland\n    \n    \n      1\n      2354404\n      2820\n      POINT (1234725.325 5048309.302)\n      11\n      Otago\n    \n    \n      2\n      2354405\n      2830\n      POINT (1235914.511 5048745.117)\n      11\n      Otago\n    \n    \n      3\n      2369113\n      3033\n      POINT (1259701.635 5076570.049)\n      9\n      West Coast\n    \n    \n      4\n      2362630\n      2749\n      POINT (1378169.600 5158491.453)\n      10\n      Canterbury\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      96\n      2408394\n      2797\n      POINT (1821014.190 5647970.666)\n      7\n      Manawatu-Wanganui\n    \n    \n      97\n      2408395\n      2757\n      POINT (1820642.909 5648331.194)\n      7\n      Manawatu-Wanganui\n    \n    \n      98\n      2408397\n      2751\n      POINT (1820659.873 5649488.235)\n      2\n      Waikato\n    \n    \n      99\n      2408406\n      2720\n      POINT (1822262.592 5650428.656)\n      2\n      Waikato\n    \n    \n      100\n      2408411\n      2732\n      POINT (1822492.184 5650492.304)\n      2\n      Waikato\n    \n  \n\n101 rows × 5 columns\n\n\n\nSecond, …\n\n\n4.3.7 Joining incongruent layers\n…\n\n\n4.3.8 Distance relations\nWhile topological relations are binary — a feature either intersects with another or does not — distance relations are continuous. The distance between two objects is calculated with the distance method. The method is applied on a GeoSeries (or a GeoDataFrame), with the argument being an individual shapely geometry. The result is a Series of pairwise distances.\nThis is illustrated in the code chunk below, which finds the distance between the three highest point in New Zealand:\n\nnz_heighest = nz_height.sort_values(by='elevation', ascending=False).iloc[:3, :]\nnz_heighest\n\n\n\n\n\n  \n    \n      \n      t50_fid\n      elevation\n      geometry\n    \n  \n  \n    \n      64\n      2372236\n      3724\n      POINT (1369317.630 5169132.284)\n    \n    \n      63\n      2372235\n      3717\n      POINT (1369512.866 5168235.616)\n    \n    \n      67\n      2372252\n      3688\n      POINT (1369381.942 5168761.875)\n    \n  \n\n\n\n\nand the geographic centroid of the Canterbury region, created in Section 4.2.1:\ncanterbury_centroid = canterbury.centroid.iloc[0]\nHere are the distances:\n\nnz_heighest.distance(canterbury_centroid)\n\n64    115539.995747\n63    115390.248038\n67    115493.594066\ndtype: float64\n\n\nTo obtain a distance matrix, i.e., a pairwise set of distances between all combinations of features in objects x and y, we need to use the .apply method. This is illustrated in the command below, which finds the distances between the first three features in nz_height and the Otago and Canterbury regions of New Zealand represented by the object co:\n\nsel = nz['Name'].str.contains('Canter|Otag')\nco = nz[sel]\nco\n\n\n\n\n\n  \n    \n      \n      Name\n      Island\n      Land_area\n      ...\n      Median_income\n      Sex_ratio\n      geometry\n    \n  \n  \n    \n      10\n      Canterbury\n      South\n      44504.499091\n      ...\n      30100\n      0.975327\n      MULTIPOLYGON (((1686901.914 535...\n    \n    \n      11\n      Otago\n      South\n      31186.309188\n      ...\n      26300\n      0.951169\n      MULTIPOLYGON (((1335204.789 512...\n    \n  \n\n2 rows × 7 columns\n\n\n\nThe distance matrix d is obtained as follows (technically speaking, this is a DataFrame). In plain language, we take the geometry from each each row in nz_height.iloc[:3, :], and apply the .distance method on co with that row as the argument:\n\nd = nz_height.iloc[:3, :].apply(lambda x: co.distance(x['geometry']), axis=1)\nd\n\n\n\n\n\n  \n    \n      \n      10\n      11\n    \n  \n  \n    \n      0\n      123537.158269\n      15497.717252\n    \n    \n      1\n      94282.773074\n      0.000000\n    \n    \n      2\n      93018.560814\n      0.000000\n    \n  \n\n\n\n\nNote that the distance between the second and third features in nz_height and the second feature in co is zero. This demonstrates the fact that distances between points and polygons refer to the distance to any part of the polygon: The second and third points in nz_height are in Otago, which can be verified by plotting them:\n\nbase = co.iloc[[1]].plot(color='none')\nnz_height.iloc[1:3, :].plot(ax=base);"
  },
  {
    "objectID": "04-spatial-operations.html#spatial-ras",
    "href": "04-spatial-operations.html#spatial-ras",
    "title": "4  Spatial data operations",
    "section": "4.4 Spatial operations on raster data",
    "text": "4.4 Spatial operations on raster data\n\n4.4.1 Spatial subsetting\nThe previous chapter (Section Section 3.4) demonstrated how to retrieve values associated with specific cell IDs or row and column combinations. Raster objects can also be extracted by location (coordinates) and other spatial objects. To use coordinates for subsetting, we can use the .sample method of a rasterio file connection object, combined with a list of coordinate tuples. The methods is demonstrated below to find the value of the cell that covers a point located at coordinates of 0.1, 0.1 in elev. The returned object is a generator:\n\nsrc_elev.sample([(0.1, 0.1)])\n\n<generator object sample_gen at 0x7fddb4aa5230>\n\n\nIn case we want all values at once we can apply list. The result is 16:\n\nlist(src_elev.sample([(0.1, 0.1)]))\n\n[array([16], dtype=uint8)]\n\n\nRaster objects can also be subset with another raster object, as demonstrated in the code chunk below:\n…\n# ...\nAnother common use case of spatial subsetting is using a boolean mask, based on another raster with the same extent and resolution, or the original one, as illustrated in Figure …. To do that, we “erase” the values in the array of one raster, according to another corresponding “mask” raster. For example, let us read the elev.tif raster array:\n\nelev = src_elev.read(1)\nelev\n\narray([[ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\nand create a correspinding random mask:\n\nnp.random.seed(1)\nmask = np.random.choice([True, False], elev.shape)\nmask\n\narray([[False, False,  True,  True, False, False],\n       [False, False, False,  True,  True, False],\n       [ True, False, False,  True,  True, False],\n       [ True,  True,  True, False,  True,  True],\n       [False,  True,  True,  True, False,  True],\n       [ True,  True, False, False, False, False]])\n\n\nIn the code chunk above, we have created a mask object called mask with values randomly assigned to True and False. Next, we want to keep those values of elev which are False in mask (i.e., they are not masked). In other words, we want to mask elev with mask. The result is stored in a copy named elev1. To be able to store np.nan in the raster, we also need to convert it to float (see Section 3.4.2):\n\nelev1 = elev.copy()\nelev1 = elev1.astype('float64')\nelev1[mask] = np.nan\nelev1\n\narray([[ 1.,  2., nan, nan,  5.,  6.],\n       [ 7.,  8.,  9., nan, nan, 12.],\n       [nan, 14., 15., nan, nan, 18.],\n       [nan, nan, nan, 22., nan, nan],\n       [25., nan, nan, nan, 29., nan],\n       [nan, nan, 33., 34., 35., 36.]])\n\n\nThe result is shown in Figure 4.3.\n\nfig, axes = plt.subplots(ncols=3, figsize=(9,5))\nshow(elev, ax=axes[0])\nshow(mask, ax=axes[1])\nshow(elev1, ax=axes[2])\naxes[0].set_title(\"Original\")\naxes[1].set_title(\"Mask\")\naxes[2].set_title(\"Result\");\n\n\n\n\nFigure 4.3: Original raster (left). Raster mask (middle). Output of masking a raster (right).\n\n\n\n\nThe above approach can be also used to replace some values (e.g., expected to be wrong) with nan:\n\nelev1 = elev.copy()\nelev1 = elev1.astype('float64')\nelev1[elev1 < 20] = np.nan\nelev1\n\narray([[nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, 20., 21., 22., 23., 24.],\n       [25., 26., 27., 28., 29., 30.],\n       [31., 32., 33., 34., 35., 36.]])\n\n\nThese operations are in fact Boolean local operations, since we compare cell-wise two rasters. The next subsection explores these and related operations in more detail.\n\n\n4.4.2 Map algebra\nThe term ‘map algebra’ was coined in the late 1970s to describe a “set of conventions, capabilities, and techniques” for the analysis of geographic raster and (although less prominently) vector data (Tomlin 1994). In this context, we define map algebra more narrowly, as operations that modify or summarise raster cell values, with reference to surrounding cells, zones, or statistical functions that apply to every cell.\nMap algebra operations tend to be fast, because raster datasets only implicitly store coordinates, hence the old adage “raster is faster but vector is corrector”. The location of cells in raster datasets can be calculated by using its matrix position and the resolution and origin of the dataset (stored in the header). For the processing, however, the geographic position of a cell is barely relevant as long as we make sure that the cell position is still the same after the processing. Additionally, if two or more raster datasets share the same extent, projection and resolution, one could treat them as matrices for the processing.\nThis is the way that map algebra works with the terra package. First, the headers of the raster datasets are queried and (in cases where map algebra operations work on more than one dataset) checked to ensure the datasets are compatible. Second, map algebra retains the so-called one-to-one locational correspondence, meaning that cells cannot move. This differs from matrix algebra, in which values change position, for example when multiplying or dividing matrices.\nMap algebra (or cartographic modeling with raster data) divides raster operations into four subclasses (Tomlin 1990), with each working on one or several grids simultaneously:\n\nLocal or per-cell operations\nFocal or neighborhood operations. Most often the output cell value is the result of a 3 x 3 input cell block\nZonal operations are similar to focal operations, but the surrounding pixel grid on which new values are computed can have irregular sizes and shapes\nGlobal or per-raster operations; that means the output cell derives its value potentially from one or several entire rasters\n\nThis typology classifies map algebra operations by the number of cells used for each pixel processing step and the type of the output. For the sake of completeness, we should mention that raster operations can also be classified by discipline such as terrain, hydrological analysis, or image classification. The following sections explain how each type of map algebra operations can be used, with reference to worked examples.\n\n\n4.4.3 Local operations\nLocal operations comprise all cell-by-cell operations in one or several layers. Raster algebra is a classical use case of local operations - this includes adding or subtracting values from a raster, squaring and multipling rasters. Raster algebra also allows logical operations such as finding all raster cells that are greater than a specific value (5 in our example below). Local operations are applied using the numpy array operations syntax, as demonstrated below:\nFirst, we need to read raster values:\n\nelev = src_elev.read(1)\nelev\n\narray([[ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\nNow, any element-wise array operation can be applied. For example:\n\nelev + elev\n\narray([[ 2,  4,  6,  8, 10, 12],\n       [14, 16, 18, 20, 22, 24],\n       [26, 28, 30, 32, 34, 36],\n       [38, 40, 42, 44, 46, 48],\n       [50, 52, 54, 56, 58, 60],\n       [62, 64, 66, 68, 70, 72]], dtype=uint8)\n\n\nFigure 4.4 demonstrates a few more examples.\n\nfig, axes = plt.subplots(ncols=4, figsize=(9,5))\nshow(elev + elev, ax=axes[0], cmap=\"Oranges\")\nshow(elev ** 2, ax=axes[1], cmap=\"Oranges\")\nshow(np.log(elev), ax=axes[2], cmap=\"Oranges\")\nshow(elev > 5, ax=axes[3], cmap=\"Oranges\")\naxes[0].set_title(\"elev+elev\")\naxes[1].set_title(\"elev ** 2\")\naxes[2].set_title(\"np.log(elev)\")\naxes[3].set_title(\"elev > 5\");\n\n\n\n\nFigure 4.4: Examples of different local operations of the elev raster object: adding two rasters, squaring, applying logarithmic transformation, and performing a logical operation.\n\n\n\n\nAnother good example of local operations is the classification of intervals of numeric values into groups such as grouping a digital elevation model into low (class 1), middle (class 2) and high elevations (class 3). Here, we assign the raster values in the ranges 0–12, 12–24 and 24–36 are reclassified to take values 1, 2 and 3, respectively.\nrecl = elev.copy()\nrecl[(elev > 0)  & (elev <= 12)] = 1\nrecl[(elev > 12) & (elev <= 24)] = 2\nrecl[(elev > 24) & (elev <= 36)] = 3\nThe reclassified result is shown in Figure 4.5.\n\nfig, axes = plt.subplots(ncols=2, figsize=(9,5))\nshow(elev, ax=axes[0], cmap=\"Oranges\")\nshow(recl, ax=axes[1], cmap=\"Oranges\")\naxes[0].set_title(\"Original\")\naxes[1].set_title(\"Reclassified\");\n\n\n\n\nFigure 4.5: Reclassifying a continuous raster into three categories.\n\n\n\n\nThe calculation of the normalized difference vegetation index (NDVI) is a well-known local (pixel-by-pixel) raster operation. It returns a raster with values between -1 and 1; positive values indicate the presence of living plants (mostly > 0.2). NDVI is calculated from red and near-infrared (NIR) bands of remotely sensed imagery, typically from satellite systems such as Landsat or Sentinel. Vegetation absorbs light heavily in the visible light spectrum, and especially in the red channel, while reflecting NIR light, explaining the NVDI formula:\n\\[NDVI=\\frac{NIR-Red} {NIR+Red}\\]\nLet’s calculate NDVI for the multispectral satellite file of the Zion National Park.\nmulti_rast = src_multi_rast.read()\nnir = multi_rast[3,:,:]\nred = multi_rast[2,:,:]\nndvi = (nir-red)/(nir+red)\nConvert values >1 to “No Data”:\nndvi[ndvi>1] = np.nan\nWhen plotting an RGB image using the show function, the function assumes that:\n\nValues are in the range [0,1] for floats, or [0,255] for integers (otherwise clipped)\nThe order of bands is RGB\n\nTo “prepare” the multi-band raster for show, we therefore reverse the order of bands (which is originally BGR+NIR), and divided by the maximum to set the maximum value at 1:\nmulti_rast_rgb = multi_rast[(2,1,0), :, :] / multi_rast.max()\nThe result is shown in Figure 4.6.\n\nfig, axes = plt.subplots(ncols=2, figsize=(9,5))\nshow(multi_rast_rgb, ax=axes[0], cmap=\"RdYlGn\")\nshow(ndvi, ax=axes[1], cmap=\"Greens\")\naxes[0].set_title(\"RGB image\")\naxes[1].set_title(\"NDVI\");\n\n\n\n\nFigure 4.6: RGB image (left) and NDVI values (right) calculated for the example satellite file of the Zion National Park.\n\n\n\n\n\n\n4.4.4 Focal operations\nWhile local functions operate on one cell, though possibly from multiple layers, focal operations take into account a central (focal) cell and its neighbors. The neighborhood (also named kernel, filter or moving window) under consideration is typically of size 3-by-3 cells (that is the central cell and its eight surrounding neighbors), but can take on any other (not necessarily rectangular) shape as defined by the user. A focal operation applies an aggregation function to all cells within the specified neighborhood, uses the corresponding output as the new value for the the central cell, and moves on to the next central cell (Figure …). Other names for this operation are spatial filtering and convolution (Burrough, McDonnell, and Lloyd 2015).\nIn Python, the scipy.ndimage package has a comprehensive collection of functions to perform filtering of numpy arrays, such as:\n\nminimum_filter\nmaximum_filter\nuniform_filter (i.e., mean filter)\nmedian_filter etc.\n\nIn this group of functions, we define the shape of the moving window with either one of:\n\nsize—a single number or tuple, implying a filter of those dimensions\nfootprint—a boolean array, representing both the window shape and the identity of elements being included\n\nIn addition to specific built-in filters,\n\nconvolve applies the sum function after multiplying by a custom weights array\ngeneric_filter makes it possible to pass any custom function, where the user can specify any type of custom window-based calculatio.\n\nFor example, here we apply the minimum filter with window size of 3 on elev:\n\nelev\n\narray([[ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\n\nelev_min = scipy.ndimage.minimum_filter(elev, size=3)\nelev_min\n\narray([[ 1,  1,  2,  3,  4,  5],\n       [ 1,  1,  2,  3,  4,  5],\n       [ 7,  7,  8,  9, 10, 11],\n       [13, 13, 14, 15, 16, 17],\n       [19, 19, 20, 21, 22, 23],\n       [25, 25, 26, 27, 28, 29]], dtype=uint8)\n\n\nSpecial care should be given to the edge pixels. How should they be calculated? scipy.ndimage gives several options through the mode parameter:\n\nreflect (the default)\nconstant\nnearest\nmirror\nwrap\n\nSometimes artificially extending raster edges is considered unsuitable. In other words, we may wish the resulting raster to contain pixel values with “complete” windows only, for example to have a uniform sample size or because values in all directions matter (such as in topographic calculations). There is no specific option not to extend edges in scipy.ndimage. However, to get the same effect, the edges of the filtered array can be assigned with nan, in a number of rows and columns according to filter size. For example, when using a filter of size=3, the first “layer” of pixels may be assigned with nan, reflecting the fact that these pixels have incomplete 3*3 neighborhoods:\n\nelev_min = elev_min.astype('float')\nelev_min[:, [0, -1]] = np.nan\nelev_min[[0, -1], :] = np.nan\nelev_min\n\narray([[nan, nan, nan, nan, nan, nan],\n       [nan,  1.,  2.,  3.,  4., nan],\n       [nan,  7.,  8.,  9., 10., nan],\n       [nan, 13., 14., 15., 16., nan],\n       [nan, 19., 20., 21., 22., nan],\n       [nan, nan, nan, nan, nan, nan]])\n\n\nWe can quickly check if the output meets our expectations. In our example, the minimum value has to be always the upper left corner of the moving window (remember we have created the input raster by row-wise incrementing the cell values by one starting at the upper left corner).\nFocal functions or filters play a dominant role in image processing. Low-pass or smoothing filters use the mean function to remove extremes. In the case of categorical data, we can replace the mean with the mode, which is the most common value. By contrast, high-pass filters accentuate features. The line detection Laplace and Sobel filters might serve as an example here.\nTerrain processing, the calculation of topographic characteristics such as slope, aspect and flow directions, relies on focal functions. The TerrainAttribute function from package richdem can be used to calculate common metrics, specified through the attrib argument, namely:\n\nslope_riserun Horn (1981) doi: 10.1109/PROC.1981.11918\nslope_percentage Horn (1981) doi: 10.1109/PROC.1981.11918\nslope_degrees Horn (1981) doi: 10.1109/PROC.1981.11918\nslope_radians Horn (1981) doi: 10.1109/PROC.1981.11918\naspect Horn (1981) doi: 10.1109/PROC.1981.11918\ncurvature Zevenbergen and Thorne (1987) doi: 10.1002/esp.3290120107\nplanform_curvature Zevenbergen and Thorne (1987) doi: 10.1002/esp.3290120107\nprofile_curvature Zevenbergen and Thorne (1987) doi: 10.1002/esp.3290120107\n\n\n\n4.4.5 Zonal operations\nJust like focal operations, zonal operations apply an aggregation function to multiple raster cells. However, a second raster, usually with categorical values, defines the zonal filters (or ‘zones’) in the case of zonal operations, as opposed to a predefined neighborhood window in the case of focal operation presented in the previous section. Consequently, raster cells defining the zonal filter do not necessarily have to be neighbors. Our grain size raster is a good example, as illustrated in the right panel of Figure 3.2: different grain sizes are spread irregularly throughout the raster. Finally, the result of a zonal operation is a summary table grouped by zone which is why this operation is also known as zonal statistics in the GIS world. This is in contrast to focal operations which return a raster object.\nTo demonstrate, let us get back to the grain and elev rasters (Figure 3.2). To calculate zonal statistics, we use the arrays with raster values. The elev array was already imported earlier:\n\nelev\n\narray([[ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\nThe grain array is imported here using .read:\n\ngrain = src_grain.read(1)\ngrain\n\narray([[1, 0, 1, 2, 2, 2],\n       [0, 2, 0, 0, 2, 1],\n       [0, 2, 2, 0, 0, 2],\n       [0, 0, 1, 1, 1, 1],\n       [1, 1, 1, 2, 1, 1],\n       [2, 1, 2, 2, 0, 2]], dtype=uint8)\n\n\nOur interntion is to calculate the average (or any other summary function, for that matter) of elevation in each zone defined by grain values. First, we can obtain the unique values defining the zones using np.unique:\n\nnp.unique(grain)\n\narray([0, 1, 2], dtype=uint8)\n\n\nNow, we can use dictionary conprehension to “split” the elev array into separate one-dimensional arrays with values per grain group, with keys being the unique grain values:\n\nz = {i: elev[grain == i] for i in np.unique(grain)}\nz\n\n{0: array([ 2,  7,  9, 10, 13, 16, 17, 19, 20, 35], dtype=uint8),\n 1: array([ 1,  3, 12, 21, 22, 23, 24, 25, 26, 27, 29, 30, 32], dtype=uint8),\n 2: array([ 4,  5,  6,  8, 11, 14, 15, 18, 28, 31, 33, 34, 36], dtype=uint8)}\n\n\nAt this stage, we can expand the dictionary comprehension expression to calculate the mean elevation associated with each grain size class. Instead of placing the elevation values (elev[grain==i]) into the dictionary values, we place their mean (elev[grain==i].mean()):\n\nz = {i: elev[grain == i].mean() for i in np.unique(grain)}\nz\n\n{0: 14.8, 1: 21.153846153846153, 2: 18.692307692307693}\n\n\nThis returns the statistics for each category, here the mean elevation for each grain size class. For example, the mean elevation in pixels characterized by grain size 0 is 14.8, and so on.\n\n\n4.4.6 Global operations and distances\n…\n\n\n4.4.7 Map algebra counterparts in vector processing\n…\n\n\n4.4.8 Merging rasters\n…"
  },
  {
    "objectID": "04-spatial-operations.html#exercises",
    "href": "04-spatial-operations.html#exercises",
    "title": "4  Spatial data operations",
    "section": "4.5 Exercises",
    "text": "4.5 Exercises\n\nWrite a function which accepts and array and an int specifying the number of rows/columns to erase along an array edges. The function needs to return the modified array with nan values along its edges."
  },
  {
    "objectID": "05-geometry-operations.html",
    "href": "05-geometry-operations.html",
    "title": "5  Geometry operations",
    "section": "",
    "text": "Packages…\nimport shapely.geometry\nimport geopandas as gpd\nimport topojson as tp\nSample data…\nseine = gpd.read_file(\"data/seine.gpkg\")\nus_states = gpd.read_file(\"data/us_states.gpkg\")\nnz = gpd.read_file(\"data/nz.gpkg\")"
  },
  {
    "objectID": "05-geometry-operations.html#introduction",
    "href": "05-geometry-operations.html#introduction",
    "title": "5  Geometry operations",
    "section": "5.2 Introduction",
    "text": "5.2 Introduction\nSo far the book has explained the structure of geographic datasets (Chapter 2), and how to manipulate them based on their non-geographic attributes (Chapter 3) and spatial relations (Chapter 4). This chapter focusses on manipulating the geographic elements of geographic objects, for example by simplifying and converting vector geometries, cropping raster datasets, and converting vector objects into rasters and from rasters into vectors. After reading it—and attempting the exercises at the end—you should understand and have control over the geometry column in sf objects and the extent and geographic location of pixels represented in rasters in relation to other geographic objects.\nSection 5.3 covers transforming vector geometries with ‘unary’ and ‘binary’ operations. Unary operations work on a single geometry in isolation, including simplification (of lines and polygons), the creation of buffers and centroids, and shifting/scaling/rotating single geometries using ‘affine transformations’ (Section 5.3.1 to Section 5.3.4). Binary transformations modify one geometry based on the shape of another, including clipping and geometry unions, covered in Section 5.3.5 and Section 5.3.7, respectively. Type transformations (from a polygon to a line, for example) are demonstrated in Section Section 5.3.8.\nSection 5.4 covers geometric transformations on raster objects. This involves changing the size and number of the underlying pixels, and assigning them new values. It teaches how to change the resolution (also called raster aggregation and disaggregation), the extent and the origin of a raster. These operations are especially useful if one would like to align raster datasets from diverse sources. Aligned raster objects share a one-to-one correspondence between pixels, allowing them to be processed using map algebra operations, described in Section 4.3.2. The final Section 6 connects vector and raster objects. It shows how raster values can be ‘masked’ and ‘extracted’ by vector geometries. Importantly it shows how to ‘polygonize’ rasters and ‘rasterize’ vector datasets, making the two data models more interchangeable."
  },
  {
    "objectID": "05-geometry-operations.html#sec-geo-vec",
    "href": "05-geometry-operations.html#sec-geo-vec",
    "title": "5  Geometry operations",
    "section": "5.3 Geometric operations on vector data",
    "text": "5.3 Geometric operations on vector data\n\n5.3.1 Simplification\nSimplify…\nseine_simp = seine.simplify(2000)  # 2000 m\nPlot:\n\nfig, axes = plt.subplots(ncols=2)\nseine.plot(ax=axes[0])\nseine_simp.plot(ax=axes[1])\naxes[0].set_title(\"Original\")\naxes[1].set_title(\"Simplified (d=2000 m)\");\n\n\n\n\nCompare number of nodes:\n\nimport sys\nsys.getsizeof(seine)       ## Original (bytes)\n\n354\n\n\n\nsys.getsizeof(seine_simp)  ## Simplified (bytes)\n\n168\n\n\nUS states example…. Transform…\nus_states2163 = us_states.to_crs(2163)\nSimplify…\nus_states_simp1 = us_states2163.simplify(100000)\nPlot…\n\nus_states_simp1.plot();\n\n\n\n\ntopo = tp.Topology(us_states2163, prequantize=False)\nus_states_simp2 = topo.toposimplify(100000).to_gdf()\n\nfig, axes = plt.subplots(ncols=3, figsize=(9,5))\nus_states2163.plot(ax=axes[0])\nus_states_simp1.plot(ax=axes[1])\nus_states_simp2.plot(ax=axes[2])\naxes[0].set_title(\"Original\")\naxes[1].set_title(\"Simplified (w/ geopandas)\")\naxes[2].set_title(\"Simplified (w/ topojson)\");\n\n\n\n\n\n\n5.3.2 Centroids\nCentroid operations identify the center of geographic objects. Like statistical measures of central tendency (including mean and median definitions of ‘average’), there are many ways to define the geographic center of an object. All of them create single point representations of more complex vector objects.\nThe most commonly used centroid operation is the geographic centroid. This type of centroid operation (often referred to as ‘the centroid’) represents the center of mass in a spatial object (think of balancing a plate on your finger). Geographic centroids have many uses, for example to create a simple point representation of complex geometries, or to estimate distances between polygons. Centroids of the geometries in a GeoSeries or a GeoDataFrame are accessible through the .centroid property, as demonstrated in the code below, which generates the geographic centroids of regions in New Zealand and tributaries to the River Seine, illustrated with black points in Figure ….\nnz_centroid = nz.centroid\nseine_centroid = seine.centroid\nSometimes the geographic centroid falls outside the boundaries of their parent objects (think of a doughnut). In such cases point on surface operations can be used to guarantee the point will be in the parent object (e.g., for labeling irregular multipolygon objects such as island states), as illustrated by the red points in Figure …. Notice that these red points always lie on their parent objects. They were created with the representative_point method, as follows:\nnz_pos = nz.representative_point()\nseine_pos = seine.representative_point()\nThe centroids and points in surface are illustrated in Figure 5.1:\n\nfig, axes = plt.subplots(ncols=2)\nbase = nz.plot(ax=axes[0], color=\"white\", edgecolor=\"lightgrey\")\nnz_centroid.plot(ax=axes[0], color=\"None\", edgecolor=\"black\")\nnz_pos.plot(ax=axes[0], color=\"None\", edgecolor=\"red\");\nbase = seine.plot(ax=axes[1], color=\"grey\")\nseine_centroid.plot(ax=axes[1], color=\"None\", edgecolor=\"black\")\nseine_pos.plot(ax=axes[1], color=\"None\", edgecolor=\"red\");\n\n\n\n\nFigure 5.1: Centroids (black) and points on surface red of New Zealand and Seine datasets.\n\n\n\n\n\n\n5.3.3 Buffers\nBuffers…\nseine_buff_5km = seine.buffer(5000)\nseine_buff_50km = seine.buffer(50000)\nPlot…\n\nfig, axes = plt.subplots(ncols=2)\nseine_buff_5km.plot(ax=axes[0], color=\"None\", edgecolor=[\"red\", \"green\", \"blue\"])\nseine_buff_50km.plot(ax=axes[1], color=\"None\", edgecolor=[\"red\", \"green\", \"blue\"])\naxes[0].set_title(\"5 km buffer\")\naxes[1].set_title(\"50 km buffer\");\n\n\n\n\n\n\n5.3.4 Affine transformations\nAffine transformations of GeoSeries can be done using the .affine_transform method, which is a wrapper around the shapely.affinity.affine_transform function. According to the documentation, a 2D affine transformation requires a six-parameter list [a,b,d,e,xoff,yoff] which represents the following equations for transforming the coordinates:\n\\[\nx' = a x + b y + x_\\mathrm{off}\n\\]\n\\[\ny' = d x + e y + y_\\mathrm{off}\n\\]\nThere are also simplified GeoSeries methods for specific scenarios:\n\nGeoSeries.rotate(angle, origin='center', use_radians=False)\nGeoSeries.scale(xfact=1.0, yfact=1.0, zfact=1.0, origin='center')\nGeoSeries.skew(angle, origin='center', use_radians=False)\nGeoSeries.translate(xoff=0.0, yoff=0.0, zoff=0.0)\n\nFor example, shifting only requires the \\(x_{off}\\) and \\(y_{off}\\), using .translate. The code below shifts the y-coordinates by 100,000 meters to the north, but leaves the x-coordinates untouched:\nnz_shift = nz[\"geometry\"].translate(0, 100000)\nScale…\nnz_scale = nz[\"geometry\"].scale(0.5, 0.5, origin=\"centroid\")\nRotate…\nnz_rotate = nz[\"geometry\"].rotate(-30, origin=\"centroid\")\nPlot…\n\nfig, axes = plt.subplots(ncols=3, figsize=(9,5))\nnz.plot(ax=axes[0], color=\"lightgrey\", edgecolor=\"darkgrey\")\nnz_shift.plot(ax=axes[0], color=\"red\", edgecolor=\"darkgrey\")\nnz.plot(ax=axes[1], color=\"lightgrey\", edgecolor=\"darkgrey\")\nnz_scale.plot(ax=axes[1], color=\"red\", edgecolor=\"darkgrey\")\nnz.plot(ax=axes[2], color=\"lightgrey\", edgecolor=\"darkgrey\")\nnz_rotate.plot(ax=axes[2], color=\"red\", edgecolor=\"darkgrey\")\naxes[0].set_title(\"Shift\")\naxes[1].set_title(\"Scale\")\naxes[2].set_title(\"Rotate\");\n\n\n\n\n\n\n5.3.5 Clipping\nSpatial clipping is a form of spatial subsetting that involves changes to the geometry columns of at least some of the affected features.\nClipping can only apply to features more complex than points: lines, polygons and their ‘multi’ equivalents. To illustrate the concept we will start with a simple example: two overlapping circles with a center point one unit away from each other and a radius of one (Figure …).\n\nx = shapely.geometry.Point((0, 0)).buffer(1)\ny = shapely.geometry.Point((1, 0)).buffer(1)\nshapely.geometry.GeometryCollection([x, y])\n\n\n\n\nImagine you want to select not one circle or the other, but the space covered by both x and y. This can be done using the .intersection method from shapely, illustrated using objects named x and y which represent the left- and right-hand circles (Figure …).\n\nx.intersection(y)\n\n\n\n\n\n\n5.3.6 Subsetting and clipping\n…\n\n\n5.3.7 Geometry unions\nAs we saw in Section …, spatial aggregation can silently dissolve the geometries of touching polygons in the same group. This is demonstrated in the code chunk below in which 49 us_states are aggregated into 4 regions using the .dissolve method:\n\nregions = us_states.dissolve(by='REGION', aggfunc='sum').reset_index()\nregions\n\n/opt/conda/envs/geocompy/lib/python3.10/site-packages/geopandas/geodataframe.py:1705: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  aggregated_data = data.groupby(**groupby_kwargs).agg(aggfunc)\n\n\n\n\n\n\n  \n    \n      \n      REGION\n      geometry\n      AREA\n      total_pop_10\n      total_pop_15\n    \n  \n  \n    \n      0\n      Midwest\n      MULTIPOLYGON (((-89.10077 36.94...\n      1.984047e+06\n      66514091.0\n      67546398.0\n    \n    \n      1\n      Norteast\n      MULTIPOLYGON (((-75.61724 39.83...\n      4.357609e+05\n      54909218.0\n      55989520.0\n    \n    \n      2\n      South\n      MULTIPOLYGON (((-81.38550 30.27...\n      2.314087e+06\n      112072990.0\n      118575377.0\n    \n    \n      3\n      West\n      MULTIPOLYGON (((-118.36998 32.8...\n      3.073145e+06\n      68444193.0\n      72264052.0\n    \n  \n\n\n\n\nThe result is shown in Figure …:\n\nfig, axes = plt.subplots(ncols=2, figsize=(10, 2.5))\nus_states.plot(ax=axes[0], column='total_pop_15', legend=True)\nregions.plot(ax=axes[1], column='total_pop_15', legend=True)\naxes[0].set_title('State')\naxes[1].set_title('Region');\n\n\n\n\nWhat is going on in terms of the geometries? Behind the scenes, both .dissolve combines the geometries and dissolve the boundaries between them using the .unary_union method. This is demonstrated in the code chunk below which creates a united western US:\n\nus_west = us_states[us_states['REGION'] == 'West']\nus_west_union = us_west['geometry'].unary_union\nus_west_union\n\n\n\n\n\n\n5.3.8 Type transformations\nTransformation of geometries, from one type to another, also known as “geometry casting”, is often required to facilitate spatial analysis. The shapely package can be used for geometry casting. The exact expression(s) depend on the specific transformation we are interested in. In general, you need to figure out the required input of the respective construstor function according to the “destination” geometry (e.g., shapely.geometry.LineString, etc.), then reshape the input of the “source” geometry into the right form to be passed to that function.\nLet’s create a \"MultiPoint\" to illustrate how geometry casting works on shapely geometry objects:\n\nmultipoint = shapely.geometry.MultiPoint([(1,1), (3,3), (5,1)])\nmultipoint\n\n\n\n\nA \"LineString\" can be created using shapely.geometry.LineString from a list of points. Consequently, a \"MultiPoint\" can be converted to a \"LineString\" by extracting the individual points into a list, then passing them to shapely.geometry.LineString:\n\nlinestring = shapely.geometry.LineString(list(multipoint.geoms))\nlinestring\n\n\n\n\nA \"Polygon\" can also be created using funtion shapely.geometry.Polygon, which acceps accepts a sequence of points. In principle, the last coordinate must be equal to the first, in order to form a closed shape. However, shapely.geometry.Polygon is able to complete the last coordinate automatically. Therefore:\n\npolygon = shapely.geometry.Polygon(list(multipoint.geoms))\npolygon\n\n\n\n\nThe source \"MultiPoint\" geometry, and the derived \"LineString\" and \"Polygon\" geometries are shown in Figure 5.2. Note that we convert the shapely geometries to GeoSeries for easier multi-panel plotting:\n\nfig, axes = plt.subplots(ncols=3, figsize=(9,5))\ngpd.GeoSeries(multipoint).plot(ax=axes[0])\ngpd.GeoSeries(linestring).plot(ax=axes[1])\ngpd.GeoSeries(polygon).plot(ax=axes[2])\naxes[0].set_title(\"MultiPoint\")\naxes[1].set_title(\"LineString\")\naxes[2].set_title(\"Polygon\");\n\n\n\n\nFigure 5.2: Examples of linestring and polygon casted from a multipoint geometry.\n\n\n\n\nConversion from multipoint to linestring is a common operation that creates a line object from ordered point observations, such as GPS measurements or geotagged media. This allows spatial operations such as the length of the path traveled. Conversion from multipoint or linestring to polygon is often used to calculate an area, for example from the set of GPS measurements taken around a lake or from the corners of a building lot.\nOur \"LineString\" geometry can be converted bact to a \"MultiPoint\" geometry by passing its coordinates directly to shapely.geometry.MultiPoint:\n\n# 'LineString' -> 'MultiPoint'\nshapely.geometry.MultiPoint(linestring.coords)\n\n\n\n\nThe \"Polygon\" (exterior) coordinates can be passed to shapely.geometry.MultiPoint as well:\n\n# 'Polygon' -> 'MultiPoint'\nshapely.geometry.MultiPoint(polygon.exterior.coords)\n\n\n\n\n…"
  },
  {
    "objectID": "05-geometry-operations.html#sec-geo-ras",
    "href": "05-geometry-operations.html#sec-geo-ras",
    "title": "5  Geometry operations",
    "section": "5.4 Geometric operations on raster data",
    "text": "5.4 Geometric operations on raster data\n\n5.4.1 Geometric intersections\n…\n\n\n5.4.2 Extent and origin\n…\n\n\n5.4.3 Aggregation and disaggregation\n…\n\n\n5.4.4 Resampling\n…"
  },
  {
    "objectID": "05-geometry-operations.html#exercises",
    "href": "05-geometry-operations.html#exercises",
    "title": "5  Geometry operations",
    "section": "5.5 Exercises",
    "text": "5.5 Exercises"
  },
  {
    "objectID": "06-raster-vector.html",
    "href": "06-raster-vector.html",
    "title": "6  Raster-vector interactions",
    "section": "",
    "text": "Let’s import the required packages:\nimport numpy as np\nimport shapely.geometry\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport rasterio\nimport rasterio.mask\nimport rasterstats\nfrom rasterio.plot import show\nimport math\nand load the sample data:\nsrc_srtm = rasterio.open('data/srtm.tif')\nsrc_nlcd = rasterio.open('data/nlcd.tif')\nsrc_grain = rasterio.open('data/grain.tif')\nsrc_elev = rasterio.open('data/elev.tif')\nzion = gpd.read_file('data/zion.gpkg')\nzion_points = gpd.read_file('data/zion_points.gpkg')\ncycle_hire_osm = gpd.read_file('data/cycle_hire_osm.gpkg')"
  },
  {
    "objectID": "06-raster-vector.html#introduction",
    "href": "06-raster-vector.html#introduction",
    "title": "6  Raster-vector interactions",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction"
  },
  {
    "objectID": "06-raster-vector.html#raster-cropping",
    "href": "06-raster-vector.html#raster-cropping",
    "title": "6  Raster-vector interactions",
    "section": "6.3 Raster cropping",
    "text": "6.3 Raster cropping\nMany geographic data projects involve integrating data from many different sources, such as remote sensing images (rasters) and administrative boundaries (vectors). Often the extent of input raster datasets is larger than the area of interest. In this case raster cropping and masking are useful for unifying the spatial extent of input data. Both operations reduce object memory use and associated computational resources for subsequent analysis steps, and may be a necessary preprocessing step before creating attractive maps involving raster data.\nWe will use two objects to illustrate raster cropping:\n\nThe srtm.tif raster representing elevation (meters above sea level) in south-western Utah\nThe zion.gpkg vector layer representing the Zion National Park\n\nBoth target and cropping objects must have the same projection. The following reprojects the vector layer zion into the CRS of the raster src_srtm:\nzion = zion.to_crs(src_srtm.crs)\nTo mask the image, i.e., convert all pixels which do not intersect with the zion polygon to “No Data”, we use the rasterio.mask.mask function as follows:\nout_image_mask, out_transform_mask = rasterio.mask.mask(\n    src_srtm, \n    zion['geometry'], \n    crop=False, \n    nodata=9999\n)\nNote that we need to specify a “No Data” value in agreement with the raster data type. Since srtm.tif is of type uint16, we choose 9999 (a positive integer that is guaranteed not to occur in the raster).\nThe result is the out_image array with the masked values:\n\nout_image_mask\n\narray([[[9999, 9999, 9999, ..., 9999, 9999, 9999],\n        [9999, 9999, 9999, ..., 9999, 9999, 9999],\n        [9999, 9999, 9999, ..., 9999, 9999, 9999],\n        ...,\n        [9999, 9999, 9999, ..., 9999, 9999, 9999],\n        [9999, 9999, 9999, ..., 9999, 9999, 9999],\n        [9999, 9999, 9999, ..., 9999, 9999, 9999]]], dtype=uint16)\n\n\nand the new out_transform:\n\nout_transform_mask\n\nAffine(0.0008333333332777796, 0.0, -113.23958321278403,\n       0.0, -0.0008333333332777843, 37.512916763165805)\n\n\nNote that masking (without cropping!) does not modify the raster spatial configuration. Therefore, the new transform is identical to the original:\n\nsrc_srtm.transform\n\nAffine(0.0008333333332777796, 0.0, -113.23958321278403,\n       0.0, -0.0008333333332777843, 37.512916763165805)\n\n\nUnfortunately, the out_image and out_transform object do not contain any information indicating that 9999 represents “No Data”. To associate the information with the raster, we must write it to file along with the corresponding metadata. For example, to write the cropped raster to file, we need to modify the “No Data” setting in the metadata:\n\nout_meta = src_srtm.meta\nout_meta.update(nodata=9999)\nout_meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint16',\n 'nodata': 9999,\n 'width': 465,\n 'height': 457,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.0008333333332777796, 0.0, -113.23958321278403,\n        0.0, -0.0008333333332777843, 37.512916763165805)}\n\n\nThen we can write the cropped raster to file:\nnew_dataset = rasterio.open('output/srtm_masked.tif', 'w', **out_meta)\nnew_dataset.write(out_image_mask)\nnew_dataset.close()\nNow we can re-import the raster:\nsrc_srtm_mask = rasterio.open('output/srtm_masked.tif')\nThe .meta property contains the nodata entry. Now, any relevant operation (such as plotting) will take “No Data” into account:\n\nsrc_srtm_mask.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint16',\n 'nodata': 9999.0,\n 'width': 465,\n 'height': 457,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.0008333333332777796, 0.0, -113.23958321278403,\n        0.0, -0.0008333333332777843, 37.512916763165805)}\n\n\nCropping means reducing the raster extent to the extent of the vector layer:\n\nTo crop and mask, we can use the same in rasterio.mask.mask expression shown above for masking, just setting crop=True instead of crop=False.\nTo just crop, without masking, we can derive the extent polygon and then crop using it.\n\nFor example, here is how we can obtain the extent polygon of zion, as a shapely geometry object:\n\nbb = zion.unary_union.envelope\nbb\n\n\n\n\nThe extent can now be used for masking. Here, we are also using the all_touched=True option so that pixels partially overlapping with the extent are included:\nout_image_crop, out_transform_crop = rasterio.mask.mask(\n    src_srtm, \n    [bb], \n    crop=True, \n    all_touched=True, \n    nodata=9999\n)\nFigure 6.1 shows the original raster, and the cropped and masked results.\n\nfig, axes = plt.subplots(ncols=3, figsize=(9,5))\nshow(src_srtm, ax=axes[0])\nzion.plot(ax=axes[0], color='none', edgecolor='black')\nshow(src_srtm_mask, ax=axes[1])\nzion.plot(ax=axes[1], color='none', edgecolor='black')\nshow(out_image_crop, transform=out_transform_crop, ax=axes[2])\nzion.plot(ax=axes[2], color='none', edgecolor='black')\naxes[0].set_title('Original')\naxes[1].set_title('Mask')\naxes[2].set_title('Crop');\n\n\n\n\nFigure 6.1: Raster masking and cropping"
  },
  {
    "objectID": "06-raster-vector.html#raster-extraction",
    "href": "06-raster-vector.html#raster-extraction",
    "title": "6  Raster-vector interactions",
    "section": "6.4 Raster extraction",
    "text": "6.4 Raster extraction\nRaster extraction is the process of identifying and returning the values associated with a ‘target’ raster at specific locations, based on a (typically vector) geographic ‘selector’ object. The reverse of raster extraction — assigning raster cell values based on vector objects — is rasterization, described in Section …\nIn the following examples, we use a third-party package called rasterstats, which is specifically aimed at extracting raster values:\n\nto points, via the rasterstats.point_query function, or\nto polygons, via the rasterstats.zonal_stats function.\n\n\n6.4.1 Extraction to points\nThe basic example is of extracting the value of a raster cell at specific points. For this purpose, we will use zion_points, which contain a sample of 30 locations within the Zion National Park (Figure …). The following expression extracts elevation values from srtm:\nresult = rasterstats.point_query(\n    zion_points, \n    src_srtm.read(1), \n    nodata = src_srtm.nodata, \n    affine = src_srtm.transform,\n    interpolate='nearest'\n)\nThe resulting object is a list of raster values, corresponding to zion_points:\n\nresult[:5]\n\n[1802, 2433, 1886, 1370, 1452]\n\n\nTo create a DataFrame with points’ IDs (one value per vector’s row) and related srtm values for each point, we need to assign it:\n\nzion_points['elev'] = result\nzion_points\n\n\n\n\n\n  \n    \n      \n      geometry\n      elev\n    \n  \n  \n    \n      0\n      POINT (-112.91587 37.20013)\n      1802\n    \n    \n      1\n      POINT (-113.09369 37.39263)\n      2433\n    \n    \n      2\n      POINT (-113.02462 37.33466)\n      1886\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      27\n      POINT (-113.03655 37.23446)\n      1372\n    \n    \n      28\n      POINT (-113.13933 37.39004)\n      1905\n    \n    \n      29\n      POINT (-113.09677 37.24237)\n      1574\n    \n  \n\n30 rows × 2 columns\n\n\n\n\n\n6.4.2 Extraction to lines\nRaster extraction also works with line selectors. Then, it extracts one value for each raster cell touched by a line. However, the line extraction approach is not recommended to obtain values along the transects as it is hard to get the correct distance between each pair of extracted raster values.\nIn this case, a better approach is to split the line into many points and then extract the values for these points. To demonstrate this, the code below creates zion_transect, a straight line going from northwest to southeast of the Zion National Park, illustrated in Figure 6.3(A) (see Section 2.2 for a recap on the vector data model):\nzion_transect = cbind(c(-113.2, -112.9), c(37.45, 37.2)) |> st_linestring() |> st_sfc(crs = crs(srtm)) |> st_sf(geometry = _)\nThe utility of extracting heights from a linear selector is illustrated by imagining that you are planning a hike. The method demonstrated below provides an ‘elevation profile’ of the route (the line does not need to be straight), useful for estimating how long it will take due to long climbs.\nThe first step is to add a unique id for each transect. Next, with the st_segmentize() function we can add points along our line(s) with a provided density (dfMaxLength) and convert them into points with st_cast().\nzion_transect$id = 1:nrow(zion_transect) zion_transect = st_segmentize(zion_transect, dfMaxLength = 250) zion_transect = st_cast(zion_transect, “POINT”)\nNow, we have a large set of points, and we want to derive a distance between the first point in our transects and each of the subsequent points. In this case, we only have one transect, but the code, in principle, should work on any number of transects:\nzion_transect = zion_transect |> group_by(id) |> mutate(dist = st_distance(geometry)[, 1])\nFinally, we can extract elevation values for each point in our transects and combine this information with our main object.\nzion_elev = terra::extract(srtm, vect(zion_transect)) zion_transect = cbind(zion_transect, zion_elev)\nThe resulting zion_transect can be used to create elevation profiles, as illustrated in Figure 6.3(B).\n\n\n6.4.3 Extraction to polygons\nThe final type of geographic vector object for raster extraction is polygons. Like lines, polygons tend to return many raster values per polygon. Typically, we generate summary statistics for raster values per polygon, for example to characterize a single region or to compare many regions. The generation of raster summary statistics, by polygons, is demonstrated in the code below, which creates a list of summary statistics (in this case a list of length 1, since there is just one polygon), again using rasterstats:\n\nrasterstats.zonal_stats(\n    zion, \n    src_srtm.read(1), \n    nodata = src_srtm.nodata, \n    affine = src_srtm.transform, \n    stats = ['mean', 'min', 'max']\n)\n\n[{'min': 1122.0, 'max': 2661.0, 'mean': 1818.211830154405}]\n\n\nThe results provide useful summaries, for example that the maximum height in the park is around 2,661 meters above see level (other summary statistics, such as standard deviation, can also be calculated in this way). Because there is only one polygon in the example a data frame with a single row is returned; however, the method works when multiple selector polygons are used.\nNote the stats argument, where we determine what type of statistics are calculated per polygon. Possible values other than 'mean', 'min', 'max' are:\n\n'count'—The number of valid (i.e., excluding “No Data”) pixels\n'nodata'—The number of pixels with ’No Data”\n'majority'—The most frequently occurring value\n'median'—The median value\n\nSee the documentation for the complete list. Additionally, the zonal_stats function accepts user-defined functions for calculating any custom statistics.\nTo count occurrences of categorical raster values within polygons, we can use masking (see Section…) combined with np.unique, as follows:\nout_image, out_transform = rasterio.mask.mask(\n    src_nlcd, \n    zion['geometry'].to_crs(src_nlcd.crs), \n    crop=False, \n    nodata=9999\n)\ncounts = np.unique(out_image, return_counts=True)\nAccording to the result, for example, pixel value 2 (“Developed” class) appears in 4205 pixels within the Zion polygon:\n\ncounts\n\n(array([ 2,  3,  4,  5,  6,  7,  8, 15], dtype=uint8),\n array([  4205,  98285, 298299, 203700,    235,     62,    679, 852742]))\n\n\nRaster to polygon extraction is illustrated in Figure 6.2.\n\nfig, axes = plt.subplots(ncols=2, figsize=(6,4))\nshow(src_srtm, ax=axes[0])\nzion.plot(ax=axes[0], color='none', edgecolor='black')\nshow(src_nlcd, ax=axes[1], cmap='Set3')\nzion.to_crs(src_nlcd.crs).plot(ax=axes[1], color='none', edgecolor='black')\naxes[0].set_title('Continuous data extraction')\naxes[1].set_title('Categorical data extraction');\n\n\n\n\nFigure 6.2: Area used for continuous (left) and categorical (right) raster extraction."
  },
  {
    "objectID": "06-raster-vector.html#rasterization",
    "href": "06-raster-vector.html#rasterization",
    "title": "6  Raster-vector interactions",
    "section": "6.5 Rasterization",
    "text": "6.5 Rasterization\nRasterization is the conversion of vector objects into their representation in raster objects. Usually, the output raster is used for quantitative analysis (e.g., analysis of terrain) or modeling. As we saw in Chapter 2 the raster data model has some characteristics that make it conducive to certain methods. Furthermore, the process of rasterization can help simplify datasets because the resulting values all have the same spatial resolution: rasterization can be seen as a special type of geographic data aggregation.\nThe rasterio package contains the rasterio.features.rasterize function for doing this work. To make it happen, we need to have the “template” grid definition, i.e., the “template” raster defining the extent, resolution and CRS of the output, in the form of out_shape (the dimensions) and transform (the transform). In case we have an existing template raster, we simply need to query its out_shape and transform. In case we create a custom template, e.g., covering the vector layer extent with specified resolution, there is some extra work to calculate the out_shape and transform (see next example).\nFurthermore, the rasterio.features.rasterize function requires the input shapes in the form for a generator of (geom, value) tuples, where:\n\ngeom is the given geometry (shapely)\nvalue is the value to be “burned” into pixels coinciding with the geometry (int or float)\n\nAgain, this will be made clear in the next example.\nThe geographic resolution of the “template” raster has a major impact on the results: if it is too low (cell size is too large), the result may miss the full geographic variability of the vector data; if it is too high, computational times may be excessive. There are no simple rules to follow when deciding an appropriate geographic resolution, which is heavily dependent on the intended use of the results. Often the target resolution is imposed on the user, for example when the output of rasterization needs to be aligned to the existing raster.\nTo demonstrate rasterization in action, we will use a template raster that has the same extent and CRS as the input vector data cycle_hire_osm_projected (a dataset on cycle hire points in London is illustrated in Figure 6.3) and spatial resolution of 1000 meters. First, we obtain the vector layer:\n\ncycle_hire_osm_projected = cycle_hire_osm.to_crs(27700)\ncycle_hire_osm_projected\n\n\n\n\n\n  \n    \n      \n      osm_id\n      name\n      capacity\n      cyclestreets_id\n      description\n      geometry\n    \n  \n  \n    \n      0\n      108539\n      Windsor Terrace\n      14.0\n      None\n      None\n      POINT (532352.033 182857.401)\n    \n    \n      1\n      598093293\n      Pancras Road, King's Cross\n      NaN\n      None\n      None\n      POINT (529846.579 183336.902)\n    \n    \n      2\n      772536185\n      Clerkenwell, Ampton Street\n      11.0\n      None\n      None\n      POINT (530633.846 182608.734)\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      537\n      5121513755\n      None\n      5.0\n      None\n      None\n      POINT (532529.479 178805.219)\n    \n    \n      538\n      5188912370\n      None\n      NaN\n      None\n      None\n      POINT (538721.459 180836.116)\n    \n    \n      539\n      5188912371\n      None\n      NaN\n      None\n      None\n      POINT (538411.339 180774.171)\n    \n  \n\n540 rows × 6 columns\n\n\n\nNext, we need to calculate the out_shape and transform of out template raster. To calculate the transform, we combine the top-left corner of the cycle_hire_osm_projected bounding box with the required resolution (e.g., 1000 \\(m\\)):\n\nbounds = cycle_hire_osm_projected.total_bounds\nbounds\n\narray([523037.00912759, 174934.55896644, 538721.458897  , 184971.16277044])\n\n\n\nres = 1000\ntransform = rasterio.transform.from_origin(\n    west=bounds[0], \n    north=bounds[3], \n    xsize=res, \n    ysize=res\n)\ntransform\n\nAffine(1000.0, 0.0, 523037.00912759494,\n       0.0, -1000.0, 184971.16277044098)\n\n\nTo calculate the out_shape, we divide the x-axis and y-axis extent by the resolution:\n\nrows = math.ceil((bounds[3] - bounds[1]) / res)\ncols = math.ceil((bounds[2] - bounds[0]) / res)\nshape = (rows, cols)\nshape\n\n(11, 16)\n\n\nNow, we can rasterize. Rasterization is a very flexible operation: the results depend not only on the nature of the template raster, but also on the type of input vector (e.g., points, polygons), the pixel “activation” method, and the function applied when there is more than one match.\nTo illustrate this flexibility we will try three different approaches to rasterization. First, we create a raster representing the presence or absence of cycle hire points (known as presence/absence rasters). In this case, we transfer the value of 1 to all pixels where at least one point falls in. To transform the point GeoDataFrame into a generator of shapely geometries and the (fixed) values, we use the following expression:\n\n((g, 1) for g in cycle_hire_osm_projected['geometry'].to_list())\n\n<generator object <genexpr> at 0x7f899ff91c40>\n\n\nTherefore, the rasterizing expression is:\nch_raster1 = rasterio.features.rasterize(\n    ((g, 1) for g in cycle_hire_osm_projected['geometry'].to_list()),\n    out_shape=shape,\n    transform=transform\n)\nThe result is a numpy array with the burned values of 1, and 0 in unaffected “pixels”:\n\nch_raster1\n\narray([[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n\n\nTo count the number of stations, we can use the fixed value of 1 combined with the merge_alg=rasterio.enums.MergeAlg.add, which means that multiple values burned into the same pixel are summed, rather than replaced keeping last (the default):\nch_raster2 = rasterio.features.rasterize(\n    ((g, 1) for g in cycle_hire_osm_projected['geometry'].to_list()),\n    out_shape=shape,\n    transform=transform,\n    merge_alg=rasterio.enums.MergeAlg.add\n)\nHere is the resulting array of counts:\n\nch_raster2\n\narray([[ 0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  0,  1,  3,  3],\n       [ 0,  0,  0,  1,  3,  3,  5,  5,  8,  9,  1,  3,  2,  6,  7,  0],\n       [ 0,  0,  0,  8,  5,  4, 11, 10, 12,  9, 11,  4,  8,  5,  4,  0],\n       [ 0,  1,  4, 10, 10, 11, 18, 16, 13, 12,  8,  6,  5,  2,  3,  0],\n       [ 3,  3,  9,  3,  5, 14, 10, 15,  9,  9,  5,  8,  0,  0, 12,  2],\n       [ 4,  5,  9, 11,  6,  7,  7,  3, 10,  9,  4,  0,  0,  0,  0,  0],\n       [ 4,  0,  7,  8,  8,  4, 11, 10,  7,  3,  0,  0,  0,  0,  0,  0],\n       [ 0,  1,  3,  0,  0,  1,  4,  0,  1,  0,  0,  0,  0,  0,  0,  0],\n       [ 0,  1,  1,  0,  1,  0,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 0,  1,  0,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n      dtype=uint8)\n\n\nThe new output, ch_raster2, shows the number of cycle hire points in each grid cell. The cycle hire locations have different numbers of bicycles described by the capacity variable, raising the question, what’s the capacity in each grid cell? To calculate that, we must sum the field (\"capacity\") rather than the fixed values of 1. This requires using an expanded generator of geometries and values, where we (1) extract both geometries and attribute values, and (2) filter out “No Data” values, as follows:\nch_raster3 = rasterio.features.rasterize(\n    ((g, v) for g, v in cycle_hire_osm_projected[['geometry', 'capacity']] \\\n        .dropna(subset='capacity')\n        .to_numpy() \\\n        .tolist()),\n    out_shape=shape,\n    transform=transform,\n    merge_alg=rasterio.enums.MergeAlg.add\n)\nHere is the code to illustrate the input point layer cycle_hire_osm_projected and the three variants of rasterizing it ch_raster1, ch_raster2, and ch_raster3 (Figure 6.3):\n\nfig, axes = plt.subplots(2, 2, figsize=(9, 9))\ncycle_hire_osm_projected.plot(ax=axes[0][0], column='capacity')\nshow(ch_raster1, transform=transform, ax=axes[0][1])\nshow(ch_raster2, transform=transform, ax=axes[1][0])\nshow(ch_raster3, transform=transform, ax=axes[1][1])\naxes[0][0].set_title('Points')\naxes[0][1].set_title('Presence/Absence')\naxes[1][0].set_title('Count')\naxes[1][1].set_title('Aggregated capacity');\n\n\n\n\nFigure 6.3: Examples of point rasterization."
  },
  {
    "objectID": "06-raster-vector.html#spatial-vectorization",
    "href": "06-raster-vector.html#spatial-vectorization",
    "title": "6  Raster-vector interactions",
    "section": "6.6 Spatial vectorization",
    "text": "6.6 Spatial vectorization\nSpatial vectorization is the counterpart of rasterization (Section …), but in the opposite direction. It involves converting spatially continuous raster data into spatially discrete vector data such as points, lines or polygons.\nThere are three standard methods to convert a raster to a vector layer:\n\nRaster to polygons\nRaster to points\nRaster to contours\n\nThe most straightforward form of vectorization is the first one, converting raster cells to polygons, where each pixel is represented by a rectangular polygon. The second method, raster to points, has the additional step of calculating polygon centroids. The third method, raster to contours, is somewhat unrelated. Let us demonstrate the three in the given order.\nThe rasterio.features.shapes function can be used to access to the raster pixel as polygon geometries, as well as raster values. The returned object is a generator, which yields geometry,value pairs. The additional transform argument is used to yield true spatial coordinates of the polygons, which is usually what we want.\nFor example, the following expression returns a generator named shapes, referring to the pixel polygons:\n\nshapes = rasterio.features.shapes(src_grain.read(), transform=src_grain.transform)\nshapes\n\n<generator object shapes at 0x7f899ff92500>\n\n\nWe can generate all shapes at once, into a list named pol, as follows:\npol = list(shapes)\nEach element in pol is a tuple of length 2, containing:\n\nThe GeoJSON-like dict representing the polygon geometry\nThe value of the pixel(s) which comprise the polygon\n\nFor example:\n\npol[0]\n\n({'type': 'Polygon',\n  'coordinates': [[(-1.5, 1.5),\n    (-1.5, 1.0),\n    (-1.0, 1.0),\n    (-1.0, 1.5),\n    (-1.5, 1.5)]]},\n 1.0)\n\n\nNote that each raster cell is converted into a polygon consisting of five coordinates, all of which are stored in memory (explaining why rasters are often fast compared with vectors!).\nTo transform the list into a GeoDataFrame, we need few more steps of data reshaping:\n\n# Create 'GeoSeries' with the polygons\ngeom = [shapely.geometry.shape(i[0]) for i in pol]\ngeom = gpd.GeoSeries(geom, crs=src_grain.crs)\n# Create 'Series' with the values\nvalues = [i[1] for i in pol]\nvalues = pd.Series(values)\n# Combine the 'Series' and 'GeoSeries' into a 'DataFrame'\nresult = gpd.GeoDataFrame({'value': values, 'geometry': geom})\nresult\n\n\n\n\n\n  \n    \n      \n      value\n      geometry\n    \n  \n  \n    \n      0\n      1.0\n      POLYGON ((-1.50000 1.50000, -1....\n    \n    \n      1\n      0.0\n      POLYGON ((-1.00000 1.50000, -1....\n    \n    \n      2\n      1.0\n      POLYGON ((-0.50000 1.50000, -0....\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      11\n      2.0\n      POLYGON ((0.00000 -0.50000, 0.5...\n    \n    \n      12\n      0.0\n      POLYGON ((0.50000 -1.00000, 0.5...\n    \n    \n      13\n      2.0\n      POLYGON ((1.00000 -1.00000, 1.0...\n    \n  \n\n14 rows × 2 columns\n\n\n\nThe resulting polygon layer is shown in Figure 6.4. As shown using the edgecolor='black' option, neighboring pixels sharing the same raster value are dissolved into larger polygons. The rasterio.features.shapes function does not offer a way to avoid this type of dissolving. One way to work around that is to convert an array with consecutive IDs, instead of the real values, to polygons, then extract the real values from the raster (similarly to the “raster to points” example, see below).\n\nresult.plot(column='value', edgecolor='black', legend=True);\n\n\n\n\nFigure 6.4: grain.tif converted to polygon layer\n\n\n\n\nTo transform raster to points, we can use rasterio.features.shapes, as in conversion to polygons, only with the addition of the .centroid method to go from polygons to their centroids. However, to avoid dissolving nearby pixels, we will actually convert a raster with consecutive IDs, then extract the “true” values by point (it is not strictly necessary in this example, since the values of elev.tif are all unique):\n\n# Prepare IDs array\nr = src_elev.read(1)\nids = r.copy()\nids = np.arange(0, r.size).reshape(r.shape).astype(np.int32)\nids\n\narray([[ 0,  1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17],\n       [18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35]], dtype=int32)\n\n\n# IDs raster to points\nshapes = rasterio.features.shapes(ids, transform=src_elev.transform)\npol = list(shapes)\ngeom = [shapely.geometry.shape(i[0]).centroid for i in pol]\ngeom = gpd.GeoSeries(geom, crs=src_elev.crs)\nresult = gpd.GeoDataFrame(geometry=geom)\n\n# Extract values to points\nresult['value'] = rasterstats.point_query(\n    result, \n    r, \n    nodata = src_elev.nodata, \n    affine = src_elev.transform,\n    interpolate='nearest'\n)\n\n/opt/conda/envs/geocompy/lib/python3.10/site-packages/rasterstats/io.py:313: UserWarning: Setting nodata to -999; specify nodata explicitly\n  warnings.warn(\"Setting nodata to -999; specify nodata explicitly\")\n\n\nThe result is shown in Figure 6.5.\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\nresult.plot(column='value', legend=True, ax=axes[0]);\nshow(src_elev, transform=src_elev.transform, ax=axes[0])\nresult.plot(column='value', legend=True, ax=axes[1]);\nshow(src_elev, cmap='Greys', ax=axes[1]);\n\n\n\n\nFigure 6.5: Raster and point representation of the elev.tif.\n\n\n\n\nTo contours…\n…\nAnother common type of spatial vectorization is the creation of contour lines representing lines of continuous height or temperatures (isotherms) for example. We will use a real-world digital elevation model (DEM) because the artificial raster elev produces parallel lines (task for the reader: verify this and explain why this happens). Contour lines can be created with the terra function as.contour(), which is itself a wrapper around filled.contour(), as demonstrated below (not shown):\nContours can also be added to existing plots with functions such as contour(), rasterVis::contourplot() or tmap::tm_iso(). As illustrated in Figure 6.8, isolines can be labelled.\nThe final type of vectorization involves conversion of rasters to polygons. This can be done with terra::as.polygons(), which converts each raster cell into a polygon consisting of five coordinates, all of which are stored in memory (explaining why rasters are often fast compared with vectors!).\nThis is illustrated below by converting the grain object into polygons and subsequently dissolving borders between polygons with the same attribute values (also see the dissolve argument in as.polygons())."
  },
  {
    "objectID": "06-raster-vector.html#exercises",
    "href": "06-raster-vector.html#exercises",
    "title": "6  Raster-vector interactions",
    "section": "6.7 Exercises",
    "text": "6.7 Exercises"
  },
  {
    "objectID": "07-reproj.html",
    "href": "07-reproj.html",
    "title": "7  Reprojecting geographic data",
    "section": "",
    "text": "Let’s import the required packages:\nimport numpy as np\nimport geopandas as gpd\nimport rasterio\nimport rasterio.warp\nfrom rasterio.plot import show\nand load the sample data:\nsrc_srtm = rasterio.open('data/srtm.tif')\nsrc_nlcd = rasterio.open('data/nlcd.tif')\nzion = gpd.read_file('data/zion.gpkg')\nworld = gpd.read_file('data/world.gpkg')"
  },
  {
    "objectID": "07-reproj.html#introduction",
    "href": "07-reproj.html#introduction",
    "title": "7  Reprojecting geographic data",
    "section": "7.2 Introduction",
    "text": "7.2 Introduction"
  },
  {
    "objectID": "07-reproj.html#coordinate-reference-systems",
    "href": "07-reproj.html#coordinate-reference-systems",
    "title": "7  Reprojecting geographic data",
    "section": "7.3 Coordinate Reference Systems",
    "text": "7.3 Coordinate Reference Systems"
  },
  {
    "objectID": "07-reproj.html#querying-and-setting-coordinate-systems",
    "href": "07-reproj.html#querying-and-setting-coordinate-systems",
    "title": "7  Reprojecting geographic data",
    "section": "7.4 Querying and setting coordinate systems",
    "text": "7.4 Querying and setting coordinate systems"
  },
  {
    "objectID": "07-reproj.html#geometry-operations-on-projected-and-unprojected-data",
    "href": "07-reproj.html#geometry-operations-on-projected-and-unprojected-data",
    "title": "7  Reprojecting geographic data",
    "section": "7.5 Geometry operations on projected and unprojected data",
    "text": "7.5 Geometry operations on projected and unprojected data"
  },
  {
    "objectID": "07-reproj.html#when-to-reproject",
    "href": "07-reproj.html#when-to-reproject",
    "title": "7  Reprojecting geographic data",
    "section": "7.6 When to reproject?",
    "text": "7.6 When to reproject?"
  },
  {
    "objectID": "07-reproj.html#which-crs-to-use",
    "href": "07-reproj.html#which-crs-to-use",
    "title": "7  Reprojecting geographic data",
    "section": "7.7 Which CRS to use?",
    "text": "7.7 Which CRS to use?"
  },
  {
    "objectID": "07-reproj.html#sec-reprojecting-vector-geometries",
    "href": "07-reproj.html#sec-reprojecting-vector-geometries",
    "title": "7  Reprojecting geographic data",
    "section": "7.8 Reprojecting vector geometries",
    "text": "7.8 Reprojecting vector geometries"
  },
  {
    "objectID": "07-reproj.html#reprojecting-raster-geometries",
    "href": "07-reproj.html#reprojecting-raster-geometries",
    "title": "7  Reprojecting geographic data",
    "section": "7.9 Reprojecting raster geometries",
    "text": "7.9 Reprojecting raster geometries\nThe projection concepts described in the previous section apply equally to rasters. However, there are important differences in reprojection of vectors and rasters: transforming a vector object involves changing the coordinates of every vertex but this does not apply to raster data. Rasters are composed of rectangular cells of the same size (expressed by map units, such as degrees or meters), so it is usually impracticable to transform coordinates of pixels separately. Raster reprojection involves creating a new raster object, often with a different number of columns and rows than the original. The attributes must subsequently be re-estimated, allowing the new pixels to be ‘filled’ with appropriate values. In other words, raster reprojection can be thought of as two separate spatial operations: a vector reprojection of the raster extent to another CRS (Section 7.8), and computation of new pixel values through resampling (Section 5.4.4). Thus in most cases when both raster and vector data are used, it is better to avoid reprojecting rasters and reproject vectors instead.\n\n\n\n\n\n\nNote\n\n\n\nReprojection of the regular rasters is also known as warping. Additionally, there is a second similar operation called “transformation”. Instead of resampling all of the values, it leaves all values intact but recomputes new coordinates for every raster cell, changing the grid geometry. For example, it could convert the input raster (a regular grid) into a curvilinear grid. The rasterio, like common raster file formats (such as GeoTIFF), does not support curvilinear grids (?).\n\n\nThe raster reprojection process is done using two functions from the rasterio.warp sub-package:\n\nrasterio.warp.calculate_default_transform\nrasterio.warp.reproject\n\nThe first function, calculate_default_transform, is used to calculate the new transformation matrix in the destination CRS, according to the source raster dimensions and bounds. Alternatively, the destination transformation matrix can be obtained from an existing raster; this is common practice when we need to align one raster with another, for instance to be able to combine them in raster algebra operations (Section 4.4.3) (see below). The second function rasterio.warp.reproject then actually calculates cell values in the destination grid, using the user-selected resampling method (such as nearest neighbor, or bilinear).\nLet’s take a look at two examples of raster transformation: using categorical and continuous data. Land cover data are usually represented by categorical maps. The nlcd.tif file provides information for a small area in Utah, USA obtained from National Land Cover Database 2011 in the NAD83 / UTM zone 12N CRS, as shown in the output of the code chunk below (only first line of output shown). We already created a connection to the nlcd.tif file, named src_nlcd:\n\nsrc_nlcd\n\n<open DatasetReader name='data/nlcd.tif' mode='r'>\n\n\nRecall that the raster transformation matrix and dimensions are accessible from the file connection as follows. This information will be required to calculate the destination transformation matrix (hereby printed collectively in a tuple):\n\nsrc_nlcd.transform, src_nlcd.width, src_nlcd.height\n\n(Affine(31.530298224786595, 0.0, 301903.344386758,\n        0.0, -31.52465870178793, 4154086.47216415),\n 1073,\n 1359)\n\n\nFirst, let’s define the destination CRS. In this case, we choose WGS84 (EPSG code 4326):\ndst_crs = 'EPSG:4326'\nNow, we are ready to claculate the destination raster transformation matrix (dst_transform), and the destination dimensions (dst_width, dst_height), as follows:\n\ndst_transform, dst_width, dst_height = rasterio.warp.calculate_default_transform(\n    src_nlcd.crs,\n    dst_crs,\n    src_nlcd.width,\n    src_nlcd.height,\n    *src_nlcd.bounds\n)\ndst_transform, dst_width, dst_height\n\n(Affine(0.0003150638330703061, 0.0, -113.24138992561787,\n        0.0, -0.0003150638330703061, 37.51912769147348),\n 1244,\n 1246)\n\n\nNote that *, in *src_nlcd.bounds, is used to unpack src_nlcd.bounds to four separate arguments, which calculate_default_transform requires:\n\nsrc_nlcd.bounds\n\nBoundingBox(left=301903.344386758, bottom=4111244.46098842, right=335735.354381954, top=4154086.47216415)\n\n\nNext, we will create the metadata file used for writing the reprojected raster to file. For convenience, we are taking the metadata of the source raster (src_nlcd.meta), making a copy (dst_kwargs), and then updating those specific properties that need to be changed. Note that the reprojection process typically creates “No Data” pixels, even when there were none in the input raster, since the raster orientation changes and the edges need to be “filled” to get back a rectangular extent. We need to specify a “No Data” value of our choice, if there is none, or use the existing source raster setting, such as 255 in this case:\n\ndst_kwargs = src_nlcd.meta.copy()\ndst_kwargs.update({\n    'crs': dst_crs,\n    'transform': dst_transform,\n    'width': dst_width,\n    'height': dst_height\n})\ndst_kwargs\n\n{'driver': 'GTiff',\n 'dtype': 'uint8',\n 'nodata': 255.0,\n 'width': 1244,\n 'height': 1246,\n 'count': 1,\n 'crs': 'EPSG:4326',\n 'transform': Affine(0.0003150638330703061, 0.0, -113.24138992561787,\n        0.0, -0.0003150638330703061, 37.51912769147348)}\n\n\nWe are ready to create the reprojected raster. Here, reprojection takes place between two file connections, meaning that the raster value arrays are not being read into memory at once. It is also possible to reproject into an in-memory ndarray object, see the documentation.\nTo write the reprojected raster, we first create a destination file connection dst_nlcd, pointing at the output file path of our choice (output/nlcd_4326.tif), using the updated metadata object created earlier (dst_kwargs):\ndst_nlcd = rasterio.open('output/nlcd_4326.tif', 'w', **dst_kwargs)\nThen, we use the rasterio.warp.reproject function to calculate and write the reprojection result into the dst_nlcd file connection. Note that the source and destination accept a “band” object, created using rasterio.band. In this case, there is just one band. If there were more bands, we would have to repeat the procedure for each band, using i instead of 1 inside a loop:\n\nrasterio.warp.reproject(\n    source=rasterio.band(src_nlcd, 1),\n    destination=rasterio.band(dst_nlcd, 1),\n    src_transform=src_nlcd.transform,\n    src_crs=src_nlcd.crs,\n    dst_transform=dst_transform,\n    dst_crs=dst_crs,\n    resampling=rasterio.warp.Resampling.nearest\n)\n\n(Band(ds=<open DatasetWriter name='output/nlcd_4326.tif' mode='w'>, bidx=1, dtype='uint8', shape=(1246, 1244)),\n Affine(0.0003150638330703061, 0.0, -113.24138992561787,\n        0.0, -0.0003150638330703061, 37.51912769147348))\n\n\nFinally, we close the file connection so that the data are actually written:\ndst_nlcd.close()\nMany properties of the new object differ from the previous one, including the number of columns and rows (and therefore number of cells), resolution (transformed from meters into degrees), and extent, as summarized again below (note that the number of categories increases from 8 to 9 because of the addition of NA values, not because a new category has been created — the land cover classes are preserved).\n\nsrc_nlcd.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint8',\n 'nodata': 255.0,\n 'width': 1073,\n 'height': 1359,\n 'count': 1,\n 'crs': CRS.from_epsg(26912),\n 'transform': Affine(31.530298224786595, 0.0, 301903.344386758,\n        0.0, -31.52465870178793, 4154086.47216415)}\n\n\n\nsrc_nlcd_4326 = rasterio.open('output/nlcd_4326.tif')\nsrc_nlcd_4326.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint8',\n 'nodata': 255.0,\n 'width': 1244,\n 'height': 1246,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.0003150638330703061, 0.0, -113.24138992561787,\n        0.0, -0.0003150638330703061, 37.51912769147348)}\n\n\nExamining the unique raster values tells us that the new raster has the same categories, plus the value 255 representing “No Data”:\n\nnp.unique(src_nlcd.read(1))\n\narray([1, 2, 3, 4, 5, 6, 7, 8], dtype=uint8)\n\n\n\nnp.unique(src_nlcd_4326.read(1))\n\narray([  1,   2,   3,   4,   5,   6,   7,   8, 255], dtype=uint8)\n\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(9,5))\nshow(src_nlcd, ax=axes[0], cmap='Set3')\nshow(src_nlcd_4326, ax=axes[1], cmap='Set3')\naxes[0].set_title('Original (EPSG:26912)')\naxes[1].set_title('Reprojected (EPSG:4326)');\n\n\n\n\nFigure 7.1: Reprojecting a categorical raster using nearest neighbor resampling\n\n\n\n\nIn the above example, we automatically calculated an optimal (i.e., most information preserving) destination grid using rasterio.warp.calculate_default_transform. This is appropriate when there are no specific requirements for the destination raster spatial properties. Namely, we are not required to otain a specific origin and resolution, but just wish to preserve the raster values as much as possible. To do that, calculate_default_transform “tries” to keep the extent and resolution of the destination raster as similar as possible to the source. In other situations, however, we need to reproject a raster into a specific “template”, so that it corresponds, for instance, with other rasters we use in the analysis. In the following code section, we reproject the nlcd.tif raster, again, buit this time using the nlcd_4326.tif reprojection result as the “template” to demonstrate this alternative workflow.\nFirst, we create a connection to our “template” raster to read its metadata:\n\ntemplate = rasterio.open('output/nlcd_4326.tif')\ntemplate.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint8',\n 'nodata': 255.0,\n 'width': 1244,\n 'height': 1246,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.0003150638330703061, 0.0, -113.24138992561787,\n        0.0, -0.0003150638330703061, 37.51912769147348)}\n\n\nThen, we create a write-mode connection to our destination raster, using this metadata, meaning that as the resampling result is going to have identical metadata as the “template”:\ndst_nlcd_2 = rasterio.open('output/nlcd_4326_2.tif', 'w', **template.meta)\nNow, we can resample and write the result:\n\nrasterio.warp.reproject(\n    source=rasterio.band(src_nlcd, 1),\n    destination=rasterio.band(dst_nlcd_2, 1),\n    src_transform=src_nlcd.transform,\n    src_crs=src_nlcd.crs,\n    dst_transform=dst_nlcd_2.transform,\n    dst_crs=dst_nlcd_2.crs,\n    resampling=rasterio.warp.Resampling.nearest\n)\n\n(Band(ds=<open DatasetWriter name='output/nlcd_4326_2.tif' mode='w'>, bidx=1, dtype='uint8', shape=(1246, 1244)),\n Affine(0.0003150638330703061, 0.0, -113.24138992561787,\n        0.0, -0.0003150638330703061, 37.51912769147348))\n\n\ndst_nlcd_2.close()\nNaturally, in this case, the outputs nlcd_4326.tif and nlcd_4326_2.tif are identical, as we used the same “template” and the same source data:\n\nd = rasterio.open('output/nlcd_4326.tif').read(1) == rasterio.open('output/nlcd_4326_2.tif').read(1)\nd\n\narray([[ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       ...,\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True]])\n\n\n\nnp.all(d)\n\nTrue\n\n\nThe difference is that in the first example we calculate the template automatically, using rasterio.warp.calculate_default_transform, while in the second example we used an existing raster as the “template”.\nImportantly, when the template raster has much more “coarse” resolution than the source raster, the:\n\nrasterio.warp.Resampling.average (for continuous rasters), or\nrasterio.warp.Resampling.mode (for categorical rasters)\n\nresampling method should be used, instead of rasterio.warp.Resampling.nearest. Otherwise, much of the data will be lost, as the “nearest” method can capture one pixel value only for each destination raster pixel.\nReprojecting continuous rasters (with numeric or, in this case, integer values) follows an almost identical procedure. This is demonstrated below with srtm.tif from the Shuttle Radar Topography Mission (SRTM), which represents height in meters above sea level (elevation) with the WGS84 CRS.\nWe will reproject this dataset into a projected CRS, but not with the nearest neighbor method which is appropriate for categorical data. Instead, we will use the bilinear method which computes the output cell value based on the four nearest cells in the original raster. The values in the projected dataset are the distance-weighted average of the values from these four cells: the closer the input cell is to the center of the output cell, the greater its weight. The following code section create a text string representing WGS 84 / UTM zone 12N, and reproject the raster into this CRS, using the bilinear method. The code is practically the same, except for changing the source and destination file names, and replacing nearest with bilinear:\ndst_crs = 'EPSG:32612'\ndst_transform, dst_width, dst_height = rasterio.warp.calculate_default_transform(\n    src_srtm.crs,\n    dst_crs,\n    src_srtm.width,\n    src_srtm.height,\n    *src_srtm.bounds\n)\ndst_kwargs = src_srtm.meta.copy()\ndst_kwargs.update({\n    'crs': dst_crs,\n    'transform': dst_transform,\n    'width': dst_width,\n    'height': dst_height\n})\ndst_srtm = rasterio.open('output/srtm_32612.tif', 'w', **dst_kwargs)\nrasterio.warp.reproject(\n    source=rasterio.band(src_srtm, 1),\n    destination=rasterio.band(dst_srtm, 1),\n    src_transform=src_srtm.transform,\n    src_crs=src_srtm.crs,\n    dst_transform=dst_transform,\n    dst_crs=dst_crs,\n    resampling=rasterio.warp.Resampling.bilinear\n)\ndst_srtm.close()\nFigure 7.2 shows the input and the reprojected SRTM rasters.\n\nfig, axes = plt.subplots(ncols=2, figsize=(9,5))\nshow(src_srtm, ax=axes[0])\nshow(rasterio.open('output/srtm_32612.tif'), ax=axes[1])\naxes[0].set_title('Original (EPSG:4326)')\naxes[1].set_title('Reprojected (EPSG:32612)');\n\n\n\n\nFigure 7.2: Reprojecting a continuous raster using bilinear resampling"
  },
  {
    "objectID": "07-reproj.html#custom-map-projections",
    "href": "07-reproj.html#custom-map-projections",
    "title": "7  Reprojecting geographic data",
    "section": "7.10 Custom map projections",
    "text": "7.10 Custom map projections"
  },
  {
    "objectID": "07-reproj.html#exercises",
    "href": "07-reproj.html#exercises",
    "title": "7  Reprojecting geographic data",
    "section": "7.11 Exercises",
    "text": "7.11 Exercises"
  },
  {
    "objectID": "08-read-write-plot.html#retrieving-open-data",
    "href": "08-read-write-plot.html#retrieving-open-data",
    "title": "8  Geographic data I/O",
    "section": "8.2 Retrieving open data",
    "text": "8.2 Retrieving open data"
  },
  {
    "objectID": "08-read-write-plot.html#geographic-data-packages",
    "href": "08-read-write-plot.html#geographic-data-packages",
    "title": "8  Geographic data I/O",
    "section": "8.3 Geographic data packages",
    "text": "8.3 Geographic data packages"
  },
  {
    "objectID": "08-read-write-plot.html#geographic-web-services",
    "href": "08-read-write-plot.html#geographic-web-services",
    "title": "8  Geographic data I/O",
    "section": "8.4 Geographic web services",
    "text": "8.4 Geographic web services"
  },
  {
    "objectID": "08-read-write-plot.html#file-formats",
    "href": "08-read-write-plot.html#file-formats",
    "title": "8  Geographic data I/O",
    "section": "8.5 File formats",
    "text": "8.5 File formats"
  },
  {
    "objectID": "08-read-write-plot.html#data-input-i",
    "href": "08-read-write-plot.html#data-input-i",
    "title": "8  Geographic data I/O",
    "section": "8.6 Data input (I)",
    "text": "8.6 Data input (I)\n\n8.6.1 Vector data\n\n\n8.6.2 Raster data"
  },
  {
    "objectID": "08-read-write-plot.html#data-output-o",
    "href": "08-read-write-plot.html#data-output-o",
    "title": "8  Geographic data I/O",
    "section": "8.7 Data output (O)",
    "text": "8.7 Data output (O)\n\n8.7.1 Vector data\n\n\n8.7.2 Raster data"
  },
  {
    "objectID": "08-read-write-plot.html#visual-outputs",
    "href": "08-read-write-plot.html#visual-outputs",
    "title": "8  Geographic data I/O",
    "section": "8.8 Visual outputs",
    "text": "8.8 Visual outputs"
  },
  {
    "objectID": "08-read-write-plot.html#exercises",
    "href": "08-read-write-plot.html#exercises",
    "title": "8  Geographic data I/O",
    "section": "8.9 Exercises",
    "text": "8.9 Exercises"
  },
  {
    "objectID": "09-mapping.html",
    "href": "09-mapping.html",
    "title": "9  Making maps with Python",
    "section": "",
    "text": "Geopandas explore has been used in previous chapters.\nWhen to focus on visualisation? At the end of geographic data processing workflows.\n\n\n\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport rasterio\nimport rasterio.plot\nnz = gpd.read_file(\"data/nz.gpkg\")"
  },
  {
    "objectID": "09-mapping.html#static-maps",
    "href": "09-mapping.html#static-maps",
    "title": "9  Making maps with Python",
    "section": "9.2 Static maps",
    "text": "9.2 Static maps\n\nFocus on matlibplot\nFirst example: NZ with fill and borders\nScary matplotlib code here…\n\n\nnz.plot(color=\"grey\");\nnz.plot(color=\"none\", edgecolor=\"blue\");\nnz.plot(color=\"grey\", edgecolor=\"blue\");\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs covered in Chapter 2, you can plot raster datasets as follows:\n\nnz_elev = rasterio.open('data/nz_elev.tif')\nrasterio.plot.show(nz_elev);\n\n\n\n\n\nYou can combine the raster and vector plotting methods shown above into a single visualisation with multiple layers as follows:\n\n\nfig, ax = plt.subplots(figsize=(5, 5))\nrasterio.plot.show(nz_elev, ax=ax)\nnz.to_crs(nz_elev.crs).plot(ax=ax, facecolor='none', edgecolor='r');\n\n\n\n\n\n9.2.1 Palettes\n\n\n9.2.2 Layers\n\n\n9.2.3 Faceted maps\n\n\n9.2.4 Exporting maps as images"
  },
  {
    "objectID": "09-mapping.html#interactive-maps",
    "href": "09-mapping.html#interactive-maps",
    "title": "9  Making maps with Python",
    "section": "9.3 Interactive maps",
    "text": "9.3 Interactive maps\n\nWhen are interactive maps useful\n\nAn interactive map is an important way to understand and interpret complex geographical information. A good interactive map enables movement across the map area, change the area of interest and provide additional context or text information. In this section we will look an interactive map based of national public transport access nodes (NaPTAN), the UK Department for Transport repository of public transport point-of-interest in England, Scotland and Wales consisting of: - bus stops and railway stations - tram, metro and underground stops - airports and ferry terminals\nWe will show how to create this may restricted to railway stations, tram stops and ferry terminals in Yorkshire. This will also match data to the National Rail customer reservation code (CRS) and timing point location (TIPLOC) attributes used in the the national rail timetable.\nIn the first code block we define a function get_databuffer that uses the requests library to download the NaPTAN data-set in CSV format to a StringIO buffer.\nimport io\nimport requests\n\ndef get_databuffer(uri, encoding='UTF-8'):\n    \"\"\"Download data from URI and returns as an StringIO buffer\"\"\"\n    r = requests.get(uri, timeout=10)\n    return io.StringIO(str(r.content, encoding))\n\n# NaPTAN data service\nURI='https://multiple-la-generator-dot-dft-add-naptan-prod.ew.r.appspot.com/v1/access-nodes?dataFormat=csv'\nBUFFER = get_databuffer(URI)\nWe then read the in-memory string-buffer into a Panda data-frame, treating the buffer as if it were a CSV file. We then extract the location data into a numpy two-dimensional array.\nimport pandas as pd\n\nDF1 = pd.read_csv(BUFFER, low_memory=False)\nDATA = DF1[['Longitude', 'Latitude']].values\nWe then convert the \\(transposed data-array\\) into a GeoSeries and use this to create a GeoDataFrame. Which we then tidy by dropping any columns that only contain invalid (pd.NA) values.\nimport geopandas as gpd\n\nPOINTS = gpd.points_from_xy(*DATA.T, crs='WGS84')\nNaPTAN = gpd.GeoDataFrame(data=DF1, geometry=POINTS)\n\nNaPTAN = NaPTAN.dropna(how='all', axis=1)\nThe next step is to create the timing-point TIPLOC data based on the StopType and a subset of the ATCOCode columns.\nNaPTAN['TIPLOC'] = ''\n# Heavy railway stations\nIDX1 = NaPTAN['StopType'] == 'RLY'\nNaPTAN.loc[IDX1, 'TIPLOC'] = NaPTAN['ATCOCode'].str[4:]\n\n# Ferrys\nIDX1 = NaPTAN['StopType'] == 'FER'\nNaPTAN.loc[IDX1, 'TIPLOC'] = NaPTAN['ATCOCode'].str[4:]\n\n# Metro and trams\nIDX1 = NaPTAN['StopType'] == 'MET'\nNaPTAN.loc[IDX1, 'TIPLOC'] = NaPTAN['ATCOCode'].str[6:]\nWe extract the heavy and light rail, or ferry locationsFrom the 435,298 rows in the NaPTAN data-frame.\nIDX1 = NaPTAN['StopType'].isin(['RLY', 'FER', 'MET'])\nSTATIONS = NaPTAN[IDX1]\nFilter columns and drop points within Yorkshire.\n\nFIELDS = ['ATCOCode', 'CommonName', 'ShortCommonName', 'LocalityName',\n          'StopType', 'Status', 'TIPLOC', 'geometry']\n\n# Clean up data-frame columns\nSTATIONS = STATIONS[FIELDS]\n\nYORKSHIRE = gpd.read_file('data/yorkshire.json').iloc[0, 0]\nIDX = STATIONS.within(YORKSHIRE)\n\nSTATIONS = STATIONS[IDX]\n\n# Write to GeoJSON\nSTATIONS.to_file('stations.geojson', driver='GeoJSON')\n# Write file to GeoPackage\n\nOUTPUT = STATIONS.copy()\nCRS = 'EPSG:32630'\nOUTPUT['geometry'] = OUTPUT['geometry'].to_crs(CRS)\nOUTPUT.to_file('stations.gpkg', driver='GPKG', layer='stations')\n\n/opt/conda/envs/geocompy/lib/python3.10/site-packages/pygeos/predicates.py:906: RuntimeWarning: invalid value encountered in within\n  return lib.within(a, b, **kwargs)\n\n\n\nHoloviews: facetted plotting\nPanel: allows you to create applications/dashboards\n\n\n9.3.1 GeoPandas explore\n\n\n9.3.2 Layers\n\n\n9.3.3 Publishing interactive maps\n\n\n9.3.4 Linking geographic and non-geographic visualisations"
  },
  {
    "objectID": "09-mapping.html#exercises",
    "href": "09-mapping.html#exercises",
    "title": "9  Making maps with Python",
    "section": "9.4 Exercises",
    "text": "9.4 Exercises"
  }
]